{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptsv1 = pd.read_csv(\"transcripts_dataset_v1.csv.gz\", compression=\"gzip\")\n",
    "transcriptsv2 = pd.read_csv(\"transcripts_dataset_v2.csv.gz\", compression=\"gzip\")\n",
    "metadata = pd.read_csv(\"metadata.tsv\", sep= \"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metadata null check:\n",
      " show_uri                     0\n",
      "show_name                    0\n",
      "show_description             2\n",
      "publisher                    0\n",
      "language                     0\n",
      "rss_link                     0\n",
      "episode_uri                  0\n",
      "episode_name                 0\n",
      "episode_description        205\n",
      "duration                     0\n",
      "show_filename_prefix         0\n",
      "episode_filename_prefix      0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# print('Transcipt v1 shape:', transcriptsv1.shape)\n",
    "# print('Transcipt v2 shape:', transcriptsv2.shape)\n",
    "# print('Metadata shape:', metadata.shape, '\\n')\n",
    "\n",
    "# print(list(transcriptsv1.columns))\n",
    "# print(list(transcriptsv2.columns))\n",
    "# print(list(metadata.columns))\n",
    "\n",
    "# print('\\n')\n",
    "\n",
    "# print('Transcript v1 null check:\\n', transcriptsv1.isna().sum(), '\\n')\n",
    "# print('Transcript v2 null check:\\n', transcriptsv2.isna().sum(), '\\n')\n",
    "print('Metadata null check:\\n', metadata.isna().sum()) # show_name 2 nulls, episode_description 205 nulls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105360, 6)\n"
     ]
    }
   ],
   "source": [
    "# concatenate transcripts dataframes\n",
    "transcripts = pd.concat([transcriptsv1, transcriptsv2])\n",
    "print(transcripts.shape)\n",
    "\n",
    "# subset metadata\n",
    "metadata = metadata[[\"show_name\", \"show_description\", \"publisher\", \"language\", \"episode_name\", \"episode_description\", \"duration\", \"show_filename_prefix\", \"episode_filename_prefix\"]]\n",
    "metadata = metadata.rename({\"episode_filename_prefix\": \"episode_id\", \"show_filename_prefix\": \"show_id\"}, axis=\"columns\")  # rename cols\n",
    "\n",
    "# remove episode_id suffix \n",
    "transcripts.episode_id = transcripts.episode_id.apply(lambda x: x.replace(\".json\", \"\"))\n",
    "transcripts = transcripts.drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "# remove whitespace\n",
    "metadata.episode_id = metadata.episode_id.apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(105360, 13)"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join data\n",
    "full_dataset = transcripts.join(metadata.set_index(\"episode_id\"), on=\"episode_id\", rsuffix=\"_trans\")\n",
    "full_dataset.isna().sum()\n",
    "\n",
    "full_dataset.shape\n",
    "full_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null inspection\n",
    "# full_dataset[full_dataset.isnull().any(axis=1)] # metadata discrepancies, episode_id present for all instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame as a gzip-compressed CSV file\n",
    "full_dataset.to_csv('transcripts_dataset_final.csv.gz', compression='gzip', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create dataset with 200 word documents (on subset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "t_data_sample = pd.read_csv(\"transcripts_sample.csv.gz\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1a1af0ee75eeea9e2e1ee996c87e7a2b11a0bebd85af04bb136d915cefc0abce"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
