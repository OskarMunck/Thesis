{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 16)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>show_id</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>transcript</th>\n",
       "      <th>avg_confidence</th>\n",
       "      <th>char_count</th>\n",
       "      <th>show_name</th>\n",
       "      <th>show_description</th>\n",
       "      <th>publisher</th>\n",
       "      <th>language</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>episode_description</th>\n",
       "      <th>duration</th>\n",
       "      <th>show_id_trans</th>\n",
       "      <th>category</th>\n",
       "      <th>pubdate</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>show_74R2UD42MRDtmeCGCpXNHA</td>\n",
       "      <td>7tYqM5F5SKtt7lFgcimgAh</td>\n",
       "      <td>I'm Daniel Williams director of active chicks ...</td>\n",
       "      <td>0.850038</td>\n",
       "      <td>11270</td>\n",
       "      <td>Inspire By Dani - The Podcast</td>\n",
       "      <td>Real and raw conversations on mental health, f...</td>\n",
       "      <td>Danielle Williams</td>\n",
       "      <td>['en']</td>\n",
       "      <td>The Best Advice My Mum Ever Gave Me</td>\n",
       "      <td>Today’s Episode I chat about what my mother sa...</td>\n",
       "      <td>13.96255</td>\n",
       "      <td>show_74R2UD42MRDtmeCGCpXNHA</td>\n",
       "      <td>Health &amp; Fitness</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2259</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>show_4NNO0yIIxzSsZTXR0XnaP7</td>\n",
       "      <td>3gaoEuBYb51UoX7zeqv9yr</td>\n",
       "      <td>We recording KP now. We are recording guys pro...</td>\n",
       "      <td>0.830722</td>\n",
       "      <td>26855</td>\n",
       "      <td>PROJECT MINDSET</td>\n",
       "      <td>PROJECT MINDSET was designed to UPLIFT, INSPIR...</td>\n",
       "      <td>PROJECT MINDSET</td>\n",
       "      <td>['en']</td>\n",
       "      <td>From A.D.D. to GOAT, Selling over a billion in...</td>\n",
       "      <td>From A.D.D. to GOAT, Selling over a billion in...</td>\n",
       "      <td>29.49965</td>\n",
       "      <td>show_4NNO0yIIxzSsZTXR0XnaP7</td>\n",
       "      <td>Business</td>\n",
       "      <td>2019-02-21</td>\n",
       "      <td>5276</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       show_id              episode_id  \\\n",
       "0  show_74R2UD42MRDtmeCGCpXNHA  7tYqM5F5SKtt7lFgcimgAh   \n",
       "1  show_4NNO0yIIxzSsZTXR0XnaP7  3gaoEuBYb51UoX7zeqv9yr   \n",
       "\n",
       "                                          transcript  avg_confidence  \\\n",
       "0  I'm Daniel Williams director of active chicks ...        0.850038   \n",
       "1  We recording KP now. We are recording guys pro...        0.830722   \n",
       "\n",
       "   char_count                       show_name  \\\n",
       "0       11270  Inspire By Dani - The Podcast    \n",
       "1       26855                 PROJECT MINDSET   \n",
       "\n",
       "                                    show_description          publisher  \\\n",
       "0  Real and raw conversations on mental health, f...  Danielle Williams   \n",
       "1  PROJECT MINDSET was designed to UPLIFT, INSPIR...    PROJECT MINDSET   \n",
       "\n",
       "  language                                       episode_name  \\\n",
       "0   ['en']               The Best Advice My Mum Ever Gave Me    \n",
       "1   ['en']  From A.D.D. to GOAT, Selling over a billion in...   \n",
       "\n",
       "                                 episode_description  duration  \\\n",
       "0  Today’s Episode I chat about what my mother sa...  13.96255   \n",
       "1  From A.D.D. to GOAT, Selling over a billion in...  29.49965   \n",
       "\n",
       "                 show_id_trans          category     pubdate  word_count  \n",
       "0  show_74R2UD42MRDtmeCGCpXNHA  Health & Fitness         NaN        2259  \n",
       "1  show_4NNO0yIIxzSsZTXR0XnaP7          Business  2019-02-21        5276  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data and check properties\n",
    "transcripts = pd.read_csv('transcripts_sample.csv.gz', compression='gzip')\n",
    "print(transcripts.shape)\n",
    "transcripts.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15000\n"
     ]
    }
   ],
   "source": [
    "# Create list of documents as input for enbeddings\n",
    "# without additional sampling\n",
    "docs = transcripts.transcript_subset.to_list()\n",
    "print(len(docs))\n",
    "\n",
    "# With additional sampling\n",
    "# sample_docs = transcripts.transcript_subset.sample(1000000, random_state =42).to_list()\n",
    "# print(len(sample_docs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google colab stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')  # This will prompt for authorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory\n",
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print(f'Your runtime has {ram_gb:.1f} gigabytes of available RAM\\n')\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT v1\n",
    "**Modelling data on batch level. Input instance size = max_sequence_length of embedding model**\n",
    "all-MiniLM-L6-v2 max_sequence_length: 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define submodels\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=15, \n",
    "    n_components=5, \n",
    "    min_dist=0.0, \n",
    "    metric='cosine')\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size = len(docs)*0.025, # Limit at 400 clusters \n",
    "    metric='euclidean', # same as cosine for normalised data\n",
    "    cluster_selection_method='eom', \n",
    "    prediction_data=False)\n",
    "\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n",
    "\n",
    "vectorizer_model = CountVectorizer(min_df=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba066f8ecf9d4ee1bd59e04319dd66b5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-07 11:13:38,485 - BERTopic - Transformed documents to Embeddings\n",
      "2023-03-07 11:13:48,761 - BERTopic - Reduced dimensionality\n",
      "2023-03-07 11:13:49,171 - BERTopic - Clustered reduced embeddings\n"
     ]
    }
   ],
   "source": [
    "# Initialize BERTopic and run\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    embedding_model=sentence_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    vectorizer_model = vectorizer_model,\n",
    "    low_memory = True,\n",
    "    calculate_probabilities=False, \n",
    "    verbose=True  # progress bar\n",
    "    )\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs)\n",
    "\n",
    "# Save model\n",
    "topic_model.save(\"BERT_v1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topic_model.save('/content/drive/MyDrive/BERT_v1')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTtopic V2\n",
    "**Modelling data on sentence level. Input instances size = 1 sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(215064, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>transcript_subset</th>\n",
       "      <th>sentence_enumerated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7tYqM5F5SKtt7lFgcimgAh</td>\n",
       "      <td>I'm Daniel Williams director of active chicks ...</td>\n",
       "      <td>0 - 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7tYqM5F5SKtt7lFgcimgAh</td>\n",
       "      <td>And when I say, you know, I'm making decisions...</td>\n",
       "      <td>25 - 50</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               episode_id                                  transcript_subset  \\\n",
       "0  7tYqM5F5SKtt7lFgcimgAh  I'm Daniel Williams director of active chicks ...   \n",
       "1  7tYqM5F5SKtt7lFgcimgAh  And when I say, you know, I'm making decisions...   \n",
       "\n",
       "  sentence_enumerated  \n",
       "0              0 - 25  \n",
       "1             25 - 50  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_25 = pd.read_csv('sentences_chunkssize_25.csv.gz', usecols=[1,2,3], compression='gzip')\n",
    "print(sentence_25.shape)\n",
    "sentence_25.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "215064"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create list of documents as input for BERTopic\n",
    "docs_sentences = sentence_25.transcript.to_list()\n",
    "len(docs_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define submodels\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=15, \n",
    "    n_components=5, \n",
    "    min_dist=0.0, \n",
    "    metric='cosine')\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=len(docs)*0.025, # Limit at 400 clusters\n",
    "    metric='euclidean', \n",
    "    cluster_selection_method='eom', \n",
    "    prediction_data=True)\n",
    "\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8fa65d8c0f9a43f1bb5bceac3a883702",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/6721 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Initialize BERTopic and run\n",
    "\n",
    "bert_v2 = BERTopic(\n",
    "    embedding_model=sentence_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    verbose=True\n",
    "    )\n",
    "\n",
    "topics, probs = bert_v2.fit_transform(docs_sentences)\n",
    "\n",
    "# save model\n",
    "bert_v2.save('BERT_v2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
