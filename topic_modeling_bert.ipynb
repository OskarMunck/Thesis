{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA, KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(343928, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>transcript_subset</th>\n",
       "      <th>words_enumerated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>7tYqM5F5SKtt7lFgcimgAh</td>\n",
       "      <td>I'm Daniel Williams director of active chicks ...</td>\n",
       "      <td>0 - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>7tYqM5F5SKtt7lFgcimgAh</td>\n",
       "      <td>you and I'm okay, but in 2010 when my partner ...</td>\n",
       "      <td>256 - 512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0              episode_id  \\\n",
       "0           0  7tYqM5F5SKtt7lFgcimgAh   \n",
       "1           1  7tYqM5F5SKtt7lFgcimgAh   \n",
       "\n",
       "                                   transcript_subset words_enumerated  \n",
       "0  I'm Daniel Williams director of active chicks ...          0 - 256  \n",
       "1  you and I'm okay, but in 2010 when my partner ...        256 - 512  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load data and check properties\n",
    "transcripts = pd.read_csv('word_transcript_256.csv.gz', compression='gzip')\n",
    "print(transcripts.shape)\n",
    "transcripts.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "343928\n"
     ]
    }
   ],
   "source": [
    "# Create list of documents as input for enbeddings\n",
    "# without additional sampling\n",
    "docs = transcripts.transcript_subset.to_list()\n",
    "print(len(docs))\n",
    "\n",
    "# With additional sampling\n",
    "# sample_docs = transcripts.transcript_subset.sample(1000000, random_state =42).to_list()\n",
    "# print(len(sample_docs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google colab stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')  # This will prompt for authorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory\n",
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print(f'Your runtime has {ram_gb:.1f} gigabytes of available RAM\\n')\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT v1\n",
    "**Modelling data on batch level. Input instance size = max_sequence_length of embedding model**\n",
    "all-MiniLM-L6-v2 max_sequence_length: 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load('embeddings_256.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mCanceled future for execute_request message before replies were done"
     ]
    }
   ],
   "source": [
    "# Define submodels\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def rescale(x, inplace=False):\n",
    "    \"\"\" Rescale an embedding so optimization will not have convergence issues.\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        x = np.array(x, copy=True)\n",
    "\n",
    "    x /= np.std(x[:, 0]) * 10000\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "# Initialize and rescale PCA embeddings\n",
    "pca_embeddings = rescale(KernelPCA(n_components=5, random_state=42).fit_transform(embeddings))\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=len(docs)*0.0125, \n",
    "    n_components=5, \n",
    "    min_dist=0.0, \n",
    "    metric='cosine',\n",
    "    init=pca_embeddings)\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size = len(docs)*0.025, # Limit at 400 clusters \n",
    "    metric='euclidean', # same as cosine for normalised data\n",
    "    cluster_selection_method='eom', \n",
    "    prediction_data=False)\n",
    "\n",
    "vectorizer_model = CountVectorizer(min_df=10, stop_words='english', ngram_range=(1,3))\n",
    "\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERTopic and run\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    # embedding_model=sentence_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    low_memory = True,\n",
    "    calculate_probabilities=False, \n",
    "    verbose=True  # progress bar\n",
    "    )\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings)\n",
    "\n",
    "# Save model\n",
    "topic_model.save(\"BERT_v1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTtopic V2\n",
    "**Modelling data on sentence level. Input instances size = 1 sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_25 = pd.read_csv('sentences_chunkssize_25.csv.gz', usecols=[1,2,3], compression='gzip')\n",
    "print(sentence_25.shape)\n",
    "sentence_25.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of documents as input for BERTopic\n",
    "docs_sentences = sentence_25.transcript.to_list()\n",
    "len(docs_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define submodels\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=15, \n",
    "    n_components=5, \n",
    "    min_dist=0.0, \n",
    "    metric='cosine')\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=len(docs)*0.025, # Limit at 400 clusters\n",
    "    metric='euclidean', \n",
    "    cluster_selection_method='eom', \n",
    "    prediction_data=True)\n",
    "\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERTopic and run\n",
    "\n",
    "bert_v2 = BERTopic(\n",
    "    embedding_model=sentence_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    verbose=True\n",
    "    )\n",
    "\n",
    "topics, probs = bert_v2.fit_transform(docs_sentences)\n",
    "\n",
    "# save model\n",
    "bert_v2.save('BERT_v2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
