{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File for modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from umap import UMAP\n",
    "from hdbscan import HDBSCAN\n",
    "from bertopic import BERTopic\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from bertopic.vectorizers import ClassTfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.decomposition import PCA, KernelPCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load data and check properties\n",
    "transcripts = pd.read_csv('word_transcript_256.csv.gz', compression='gzip')\n",
    "print(transcripts.shape)\n",
    "transcripts.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of documents as input for enbeddings\n",
    "# without additional sampling\n",
    "docs = transcripts.transcript_subset.to_list()\n",
    "print(len(docs))\n",
    "\n",
    "# With additional sampling\n",
    "# sample_docs = transcripts.transcript_subset.sample(1000000, random_state =42).to_list()\n",
    "# print(len(sample_docs))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Google colab stuff"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Connect to google drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')  # This will prompt for authorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check memory\n",
    "from psutil import virtual_memory\n",
    "ram_gb = virtual_memory().total / 1e9\n",
    "print(f'Your runtime has {ram_gb:.1f} gigabytes of available RAM\\n')\n",
    "\n",
    "if ram_gb < 20:\n",
    "  print('Not using a high-RAM runtime')\n",
    "else:\n",
    "  print('You are using a high-RAM runtime!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check GPU\n",
    "gpu_info = !nvidia-smi\n",
    "gpu_info = '\\n'.join(gpu_info)\n",
    "if gpu_info.find('failed') >= 0:\n",
    "  print('Not connected to a GPU')\n",
    "else:\n",
    "  print(gpu_info)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERT v1\n",
    "**Modelling data on batch level. Input instance size = max_sequence_length of embedding model**\n",
    "all-MiniLM-L6-v2 max_sequence_length: 256\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings = np.load('embeddings_256.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define submodels\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "def rescale(x, inplace=False):\n",
    "    \"\"\" Rescale an embedding so optimization will not have convergence issues.\n",
    "    \"\"\"\n",
    "    if not inplace:\n",
    "        x = np.array(x, copy=True)\n",
    "\n",
    "    x /= np.std(x[:, 0]) * 10000\n",
    "\n",
    "    return x\n",
    "\n",
    "# Initialize and rescale PCA embeddings\n",
    "pca_embeddings = rescale(KernelPCA(kernel='rbf', n_components=50, random_state=42).fit_transform(embeddings))\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=len(docs)*0.0125, \n",
    "    n_components=5, \n",
    "    min_dist=0.0, \n",
    "    metric='cosine',\n",
    "    init=pca_embeddings)\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size = len(docs)*0.025, # Limit at 400 clusters \n",
    "    metric='euclidean', # same as cosine for normalised data\n",
    "    cluster_selection_method='eom', \n",
    "    prediction_data=False)\n",
    "\n",
    "vectorizer_model = CountVectorizer(min_df=10, stop_words='english', ngram_range=(1,3))\n",
    "\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_embeddings.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERTopic and run\n",
    "\n",
    "topic_model = BERTopic(\n",
    "    # embedding_model=sentence_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    vectorizer_model=vectorizer_model\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    low_memory = True,\n",
    "    calculate_probabilities=False, \n",
    "    verbose=True  # progress bar\n",
    ")\n",
    "\n",
    "topics, probs = topic_model.fit_transform(docs, embeddings)\n",
    "\n",
    "# Save model\n",
    "topic_model.save(\"BERT_v1\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## BERTtopic V2\n",
    "**Modelling data on sentence level. Input instances size = 1 sentence**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_25 = pd.read_csv('sentences_chunkssize_25.csv.gz', usecols=[1,2,3], compression='gzip')\n",
    "print(sentence_25.shape)\n",
    "sentence_25.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create list of documents as input for BERTopic\n",
    "docs_sentences = sentence_25.transcript.to_list()\n",
    "len(docs_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define submodels\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "umap_model = UMAP(\n",
    "    n_neighbors=15, \n",
    "    n_components=5, \n",
    "    min_dist=0.0, \n",
    "    metric='cosine')\n",
    "\n",
    "hdbscan_model = HDBSCAN(\n",
    "    min_cluster_size=len(docs)*0.025, # Limit at 400 clusters\n",
    "    metric='euclidean', \n",
    "    cluster_selection_method='eom', \n",
    "    prediction_data=True)\n",
    "\n",
    "ctfidf_model = ClassTfidfTransformer(reduce_frequent_words=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize BERTopic and run\n",
    "\n",
    "bert_v2 = BERTopic(\n",
    "    embedding_model=sentence_model,\n",
    "    umap_model=umap_model,\n",
    "    hdbscan_model=hdbscan_model,\n",
    "    ctfidf_model=ctfidf_model,\n",
    "    verbose=True\n",
    "    )\n",
    "\n",
    "topics, probs = bert_v2.fit_transform(docs_sentences)\n",
    "\n",
    "# save model\n",
    "bert_v2.save('BERT_v2')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
