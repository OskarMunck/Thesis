{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# File for creating more features - not used in final version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# NLP libs\n",
    "import spacy, nltk, gensim, sklearn\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "# meta_data = pd.read_csv(\"metadata.tsv\", sep = \"/t\")\n",
    "# meta_data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = [\"show_name\", \"show_description\", \"publisher\", \"language\", \"episode_name\", \"episode_description\", \"duration\"]\n",
    "features_df = meta_data[features]\n",
    "features_df = features_df.dropna()  # drops 207 rows "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Remove newlines (initial analysis showed that eps descriptions do not contain newlines)\n",
    "# episode_description_new = [\" \".join(desc.split()) for desc in features_df.episode_description]\n",
    "# features_df[\"episode_description_clean\"] = episode_description_new # append df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### OBS: TAKES A LOT OF TIME! Approx. 60 min\n",
    "# Make spacy objects according to English language model\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# spacy_obj = [nlp(i) for i in features_df.episode_description]\n",
    "features_df[\"eps_nlp\"] = spacy_obj\n",
    "# spacy_obj_show = [nlp(i) for i in features_df.show_description]\n",
    "features_df[\"show_nlp\"] = spacy_obj_show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise episodes\n",
    "eps_tokens_all = []\n",
    "for episode in features_df.eps_nlp:\n",
    "    eps_tokens_all.append([tok.text for tok in episode])\n",
    "\n",
    "features_df[\"episode_tokens\"] = eps_tokens_all\n",
    "\n",
    "print(features_df.eps_nlp[0])\n",
    "print(features_df.episode_tokens[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenise show\n",
    "eps_tokens_all_show = []\n",
    "for show in features_df.eps_nlp:\n",
    "    eps_tokens_all_show.append([tok.text for tok in show])\n",
    "\n",
    "features_df[\"show_tokens\"] = eps_tokens_all_show\n",
    "\n",
    "print(features_df.show_nlp[0])\n",
    "print(features_df.show_tokens[0])\n",
    "features_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add word count features\n",
    "# Epsiode desc\n",
    "eps_word_count = []\n",
    "for desc in features_df.episode_tokens:\n",
    "    eps_word_count.append(len(desc))\n",
    "\n",
    "features_df[\"eps_word_count\"] = eps_word_count\n",
    "\n",
    "# Show desc\n",
    "show_word_count = []\n",
    "for desc in features_df.episode_tokens:\n",
    "    show_word_count.append(len(desc))\n",
    "\n",
    "features_df[\"show_word_count\"] = show_word_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentiment analysis\n",
    "sent_analyser = SentimentIntensityAnalyzer()\n",
    "\n",
    "def sentiment(text, choice):\n",
    "    scores = sent_analyser.polarity_scores(text)\n",
    "    return scores[choice]\n",
    "\n",
    "features_nlp[\"positive_score_eps\"] = features_nlp.episode_description.apply(lambda x: sentiment(x, \"pos\"))\n",
    "features_nlp[\"negative_score_eps\"] = features_nlp.episode_description.apply(lambda x: sentiment(x, \"neg\"))\n",
    "features_nlp[\"neutral_score_eps\"] = features_nlp.episode_description.apply(lambda x: sentiment(x, \"neu\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Up until this point, the metadata_features dataset is constructed**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_nlp = pd.read_csv('metadata_features.csv.gz', sep = \",\", compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write to file to avoid long processing in the future\n",
    "# features_nlp.to_csv(\"metadata_features.csv.gz\", index=False, compression='gzip', encoding='utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2703f8c9ab1ac04b22966d975602a3b2e9be2d364f571b124b77afe9344d7a81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
