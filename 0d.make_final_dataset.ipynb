{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Join all data from previous steps to one file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "transcriptsv1 = pd.read_csv(\"transcripts_dataset_v1.csv.gz\", compression=\"gzip\")\n",
    "transcriptsv2 = pd.read_csv(\"transcripts_dataset_v2.csv.gz\", compression=\"gzip\")\n",
    "metadata = pd.read_csv(\"metadata.csv.gz\", compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transcipt v1 shape: (39892, 6)\n",
      "Transcipt v2 shape: (65468, 6)\n",
      "Metadata shape: (105360, 14) \n",
      "\n",
      "Metadata null check:\n",
      " show_uri                       0\n",
      "show_name                      0\n",
      "show_description               2\n",
      "publisher                      0\n",
      "language                       0\n",
      "rss_link                       0\n",
      "episode_uri                    0\n",
      "episode_name                   0\n",
      "episode_description          205\n",
      "duration                       0\n",
      "show_filename_prefix           0\n",
      "episode_filename_prefix        0\n",
      "category                    6000\n",
      "pubdate                    20030\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print('Transcipt v1 shape:', transcriptsv1.shape)\n",
    "print('Transcipt v2 shape:', transcriptsv2.shape)\n",
    "print('Metadata shape:', metadata.shape, '\\n')\n",
    "\n",
    "# print(list(transcriptsv1.columns))\n",
    "# print(list(transcriptsv2.columns))\n",
    "# print(list(metadata.columns))\n",
    "\n",
    "# print('\\n')\n",
    "\n",
    "# print('Transcript v1 null check:\\n', transcriptsv1.isna().sum(), '\\n')\n",
    "# print('Transcript v2 null check:\\n', transcriptsv2.isna().sum(), '\\n')\n",
    "print('Metadata null check:\\n', metadata.isna().sum()) # show_name 2 nulls, episode_description 205 nulls, category 6000, pubdate 20030\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105360, 6)\n"
     ]
    }
   ],
   "source": [
    "# concatenate transcripts dataframes\n",
    "transcripts = pd.concat([transcriptsv1, transcriptsv2])\n",
    "print(transcripts.shape)\n",
    "\n",
    "# subset metadata\n",
    "metadata = metadata[[\"show_name\", \"show_description\", \"publisher\", \"language\", \"episode_name\", \"episode_description\", \"duration\", \"show_filename_prefix\", \"episode_filename_prefix\", 'category', 'pubdate']]\n",
    "metadata = metadata.rename({\"episode_filename_prefix\": \"episode_id\", \"show_filename_prefix\": \"show_id\"}, axis=\"columns\")  # rename cols\n",
    "\n",
    "# remove episode_id suffix \n",
    "transcripts.episode_id = transcripts.episode_id.apply(lambda x: x.replace(\".json\", \"\"))\n",
    "transcripts = transcripts.drop(\"Unnamed: 0\", axis=1)\n",
    "\n",
    "# remove whitespace\n",
    "metadata.episode_id = metadata.episode_id.apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "show_id                    0\n",
       "episode_id                 0\n",
       "transcript                 0\n",
       "avg_confidence             0\n",
       "word_count                 0\n",
       "show_name                  0\n",
       "show_description           2\n",
       "publisher                  0\n",
       "language                   0\n",
       "episode_name               0\n",
       "episode_description      205\n",
       "duration                   0\n",
       "show_id_trans              0\n",
       "category                6000\n",
       "pubdate                20030\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# join data\n",
    "full_dataset = transcripts.join(metadata.set_index(\"episode_id\"), on=\"episode_id\", rsuffix=\"_trans\")\n",
    "full_dataset.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105360, 15)\n"
     ]
    }
   ],
   "source": [
    "print(full_dataset.shape)\n",
    "# full_dataset.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Null inspection\n",
    "full_dataset[full_dataset.isnull().any(axis=1)] # metadata discrepancies, episode_id present for all instances"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correct mistake in word count from directory walk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# word_count was actually char count \n",
    "full_dataset = full_dataset.rename(columns={\"word_count\": \"char_count\"})\n",
    "\n",
    "# count words\n",
    "full_dataset[\"word_count\"] = full_dataset.transcript.apply(lambda x: len(x.split(\" \")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "languages_not_keep = [\"['nl-BE']\",\"['hi']\",\"['id']\",\"['ml']\",\"['ms']\",\"['pt']\",\"['en-JM']\",\"['en-IN']\",\"['ga']\",\"['ta']\",\"['es']\"]\n",
    "languages = list(set(full_dataset.language.unique()) - set(languages_not_keep))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(105228, 16)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['en']       79452\n",
       "['en-US']    20264\n",
       "['en-GB']     2335\n",
       "['en-AU']     1377\n",
       "['en-CA']      962\n",
       "['en-PH']      368\n",
       "['en-IE']      243\n",
       "['en-NZ']      141\n",
       "['en-ZA']       86\n",
       "Name: language, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_dataset = full_dataset[full_dataset['language'].isin(languages)]\n",
    "print(full_dataset.shape)\n",
    "full_dataset.language.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame as a gzip-compressed CSV file\n",
    "# full_dataset.to_csv('transcripts_dataset_final.csv.gz', compression='gzip', index=False)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Filter out Education and Sports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['show_id', 'episode_id', 'transcript', 'avg_confidence', 'char_count',\n",
      "       'show_name', 'show_description', 'publisher', 'language',\n",
      "       'episode_name', 'episode_description', 'duration', 'show_id_trans',\n",
      "       'category', 'pubdate', 'word_count'],\n",
      "      dtype='object')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(105228, 16)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts_full = pd.read_csv(\"transcripts_dataset_final.csv.gz\", compression=\"gzip\")\n",
    "print(transcripts_full.columns)\n",
    "transcripts_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "education_transcripts = transcripts_full[transcripts_full.category == 'Education']\n",
    "education_transcripts.shape\n",
    "# education_transcripts.to_csv('education_transcripts.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_transcripts = transcripts_full[transcripts_full.category == 'Sports']\n",
    "sports_transcripts.shape\n",
    "# sports_transcripts.to_csv('sports_transcripts.csv.gz', compression='gzip')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "2703f8c9ab1ac04b22966d975602a3b2e9be2d364f571b124b77afe9344d7a81"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
