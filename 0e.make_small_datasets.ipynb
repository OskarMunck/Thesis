{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def tokenize_split(text):\n",
    "    \"\"\"simple tokeniser\"\"\"\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def word_chunk_transcript(transcripts, name_variable='transcript', chunk_size=500):\n",
    "    \"\"\" transcripts: must have two columns - episode_id and tokenised_transcript\n",
    "        chunk_size: number of tokens in chunk \n",
    "    \"\"\"\n",
    "    transcripts[\"tokenised_transcript\"] = transcripts[name_variable].apply(lambda x: tokenize_split(x))\n",
    "\n",
    "    episode_ls = []\n",
    "    transcript_ls = []\n",
    "    words_enum_ls = [] \n",
    "    for index, row in transcripts.iterrows():\n",
    "        for i in range(0, len(row[\"tokenised_transcript\"]), chunk_size):\n",
    "            episode_ls.append(row[\"episode_id\"])\n",
    "            transcript_ls.append(\" \".join(row[\"tokenised_transcript\"][i:i+chunk_size]))\n",
    "            words_enum_ls.append(f\"{i+1} - {i+chunk_size}\")\n",
    "    word_chunked_df = pd.DataFrame(data = {'episode_id': episode_ls, 'transcript_subset':transcript_ls, 'words_enumerated':words_enum_ls})\n",
    "    return word_chunked_df\n",
    "\n",
    "\n",
    "def sentence_chunk_transcript(transcripts, name_variable='transcript', chunk_size=1):\n",
    "    \"\"\" transcripts: must have two columns - episode_id and transcript\n",
    "        chunk_size: number of sentences in chunk \n",
    "    \"\"\"     \n",
    "    transcripts[\"sentence_token\"] = transcripts[name_variable].apply(lambda x: sent_tokenize(x, language='english'))\n",
    "    episode_ls = []\n",
    "    transcript_ls = []\n",
    "    sent_enum_ls = []\n",
    "    for index, row in transcripts.iterrows():\n",
    "        for i in range(0, len(row[\"sentence_token\"]), chunk_size):\n",
    "            episode_ls.append(row[\"episode_id\"])\n",
    "            transcript_ls.append(\" \".join(row[\"sentence_token\"][i:i+chunk_size]))\n",
    "            sent_enum_ls.append(i+1)\n",
    "    sentence_chunked_df = pd.DataFrame(data = {'episode_id': episode_ls, 'transcript_subset':transcript_ls, 'sentence_enumerated':sent_enum_ls})\n",
    "\n",
    "    return sentence_chunked_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make chunked dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11821, 17)\n"
     ]
    }
   ],
   "source": [
    "transcripts_sample = pd.read_csv('sports_transcripts.csv.gz', compression='gzip')\n",
    "print(transcripts_sample.shape)\n",
    "# transcripts_sample.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract columns for conversion\n",
    "cols_subset = transcripts_sample.loc[: ,[\"episode_id\", \"transcript\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(368835, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>transcript_subset</th>\n",
       "      <th>words_enumerated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41JbXYp7c2uuJoFB4TcQtD</td>\n",
       "      <td>Hello and welcome to the law review podcast. M...</td>\n",
       "      <td>0 - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41JbXYp7c2uuJoFB4TcQtD</td>\n",
       "      <td>as well as several other changes, so we'll get...</td>\n",
       "      <td>256 - 512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               episode_id                                  transcript_subset  \\\n",
       "0  41JbXYp7c2uuJoFB4TcQtD  Hello and welcome to the law review podcast. M...   \n",
       "1  41JbXYp7c2uuJoFB4TcQtD  as well as several other changes, so we'll get...   \n",
       "\n",
       "  words_enumerated  \n",
       "0          0 - 256  \n",
       "1        256 - 512  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sports_256 = word_chunk_transcript(cols_subset, name_variable='transcript', chunk_size=256)\n",
    "\n",
    "print(sports_256.shape)\n",
    "sports_256.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_256.to_csv('sports_word_256.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5460190, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>transcript_subset</th>\n",
       "      <th>sentence_enumerated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41JbXYp7c2uuJoFB4TcQtD</td>\n",
       "      <td>Hello and welcome to the law review podcast.</td>\n",
       "      <td>0 - 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41JbXYp7c2uuJoFB4TcQtD</td>\n",
       "      <td>My name is Nathan Church Droid by my partner c...</td>\n",
       "      <td>1 - 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               episode_id                                  transcript_subset  \\\n",
       "0  41JbXYp7c2uuJoFB4TcQtD       Hello and welcome to the law review podcast.   \n",
       "1  41JbXYp7c2uuJoFB4TcQtD  My name is Nathan Church Droid by my partner c...   \n",
       "\n",
       "  sentence_enumerated  \n",
       "0               0 - 1  \n",
       "1               1 - 2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sport_sent_1 = sentence_chunk_transcript(cols_subset, name_variable='transcript', chunk_size=1)\n",
    "print(sport_sent_1.shape)\n",
    "sport_sent_1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sport_sent_1 = sport_sent_1.sample(500000, random_state=42)\n",
    "sport_sent_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sport_sent_1.to_csv('sport_sent_1.csv.gz', compression='gzip')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Annotation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word count of annotated transcripts: 8023\n"
     ]
    }
   ],
   "source": [
    "sports_trans = pd.read_csv('sports_transcripts.csv.gz', compression='gzip', usecols=[2,3,16])\n",
    "sports_trans = sports_trans.sample(200)\n",
    "print(f'Average word count of annotated transcripts: {sports_trans.word_count.mean():.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write transcripts to files\n",
    "import os.path\n",
    "\n",
    "for i in range(len(sports_trans.index)):\n",
    "    save_path = '../Thesis/annotated_transcripts/'\n",
    "    name_of_file = f'transcript{i+1}_{sports_trans.iloc[i,0]}'\n",
    "    complete_path = os.path.join(save_path, name_of_file+\".txt\")\n",
    "\n",
    "    text_file = open(complete_path, \"w\")\n",
    "    text_file.write(sports_trans.iloc[i,1])\n",
    "    text_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read back and output list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "directory = '../Thesis/annotated_transcripts'\n",
    "\n",
    "annotated_ls = []\n",
    "episode_ls = []\n",
    "for dirpath, dirnames, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        episode_ls.append(file)\n",
    "        path = os.path.join(dirpath, file)\n",
    "        with open(path, errors='replace') as f: \n",
    "            lines = f.readlines()\n",
    "            annotated_ls.append(''.join(lines))\n",
    "\n",
    "annotated_df = pd.DataFrame({'transcript': annotated_ls, 'episode_id': episode_ls})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>4y67J0Fmgm5L7TPPsUunwo</td>\n",
       "      <td>[18, 22, 43, 57, 71, 91, 97, 116, 155, 181, 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>53DrbE5nPJskpPT0PtOi9O</td>\n",
       "      <td>[4, 8, 23, 39, 45, 55, 64, 71]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index              episode_id  \\\n",
       "0      0  4y67J0Fmgm5L7TPPsUunwo   \n",
       "1      0  53DrbE5nPJskpPT0PtOi9O   \n",
       "\n",
       "                                          annotation  \n",
       "0  [18, 22, 43, 57, 71, 91, 97, 116, 155, 181, 20...  \n",
       "1                     [4, 8, 23, 39, 45, 55, 64, 71]  "
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Chunk per sentence to be able to find index of segments \n",
    "df_per_transcript = sentence_chunk_transcript(annotated_df)\n",
    "\n",
    "# Find indicies\n",
    "unique_id = df_per_transcript.episode_id.unique()\n",
    "\n",
    "annotation_values = pd.DataFrame(columns=['episode_id', 'annotation'])\n",
    "\n",
    "for id in unique_id:\n",
    "    ids = id[:-4].split('_')\n",
    "    find_annotation = df_per_transcript[df_per_transcript.episode_id == id].copy()\n",
    "    annotated_index_ls = []\n",
    "    for row in find_annotation.itertuples():\n",
    "        if row[2].startswith('@@'):\n",
    "            annotated_index_ls.append(row[3])\n",
    "    if len(annotated_index_ls) != 0:\n",
    "        temp_df = pd.DataFrame({'episode_id': ids[1], 'annotation':[annotated_index_ls]})\n",
    "        annotation_values = pd.concat([annotation_values, temp_df], axis=0)\n",
    "\n",
    "annotation_values = annotation_values.reset_index()\n",
    "\n",
    "print(annotation_values.shape)\n",
    "annotation_values.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "annotation_values.to_csv('annotations.csv')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the same transcripts, un-annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(20, 3)\n"
     ]
    }
   ],
   "source": [
    "sports_trans = pd.read_csv('sports_transcripts.csv.gz', compression='gzip', usecols=[2,3,16])\n",
    "annotated_transcripts_plain = sports_trans[sports_trans.episode_id.isin(annotation_values.episode_id)].copy()\n",
    "print(annotated_transcripts_plain.shape)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>show_id</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>transcript</th>\n",
       "      <th>avg_confidence</th>\n",
       "      <th>char_count</th>\n",
       "      <th>show_name</th>\n",
       "      <th>show_description</th>\n",
       "      <th>publisher</th>\n",
       "      <th>language</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>episode_description</th>\n",
       "      <th>duration</th>\n",
       "      <th>show_id_trans</th>\n",
       "      <th>category</th>\n",
       "      <th>pubdate</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>3701</td>\n",
       "      <td>show_2z1LtWVnflRUONFAo0FADb</td>\n",
       "      <td>6preEOWrgR9eRr938upFgv</td>\n",
       "      <td>I were back. It's another Carolina podcast. Ha...</td>\n",
       "      <td>0.820242</td>\n",
       "      <td>60218</td>\n",
       "      <td>Gamecock Central Podcast Network</td>\n",
       "      <td>The Gamecock Central Podcast Network brings yo...</td>\n",
       "      <td>Gamecock Central Podcasts</td>\n",
       "      <td>['en']</td>\n",
       "      <td>Another Carolina Podcast: Early Enrollees and ...</td>\n",
       "      <td>Wes Mitchell and Chris Clark join host Pearson...</td>\n",
       "      <td>55.61905</td>\n",
       "      <td>show_2z1LtWVnflRUONFAo0FADb</td>\n",
       "      <td>Sports</td>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>11390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                      show_id              episode_id  \\\n",
       "384        3701  show_2z1LtWVnflRUONFAo0FADb  6preEOWrgR9eRr938upFgv   \n",
       "\n",
       "                                            transcript  avg_confidence  \\\n",
       "384  I were back. It's another Carolina podcast. Ha...        0.820242   \n",
       "\n",
       "     char_count                         show_name  \\\n",
       "384       60218  Gamecock Central Podcast Network   \n",
       "\n",
       "                                      show_description  \\\n",
       "384  The Gamecock Central Podcast Network brings yo...   \n",
       "\n",
       "                     publisher language  \\\n",
       "384  Gamecock Central Podcasts   ['en']   \n",
       "\n",
       "                                          episode_name  \\\n",
       "384  Another Carolina Podcast: Early Enrollees and ...   \n",
       "\n",
       "                                   episode_description  duration  \\\n",
       "384  Wes Mitchell and Chris Clark join host Pearson...  55.61905   \n",
       "\n",
       "                   show_id_trans category     pubdate  word_count  \n",
       "384  show_2z1LtWVnflRUONFAo0FADb   Sports  2020-01-08       11390  "
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv('sports_transcripts.csv.gz', compression='gzip')\n",
    "data_to_use = data[data.episode_id == '6preEOWrgR9eRr938upFgv'].copy()\n",
    "data_to_use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prediction_data = sentence_chunk_transcript(data_to_use, name_variable='transcript', chunk_size=1)\n",
    "prediction_data.to_csv('prediction_data.csv')\n",
    "print(prediction_data.shape)\n",
    "prediction_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13NDTKL5ZGs8cb8dojW3bz (224, 3)\n",
      "6preEOWrgR9eRr938upFgv (599, 3)\n",
      "0bXWB28GwN8OiqC1ykRrRX (563, 3)\n",
      "19W5dgUcFseQZBmcVF4coc (191, 3)\n",
      "3RT2j2BG8ILNYKjxsNhfvZ (368, 3)\n",
      "7A7swZJL0AtFghauiGLadV (98, 3)\n",
      "4y67J0Fmgm5L7TPPsUunwo (270, 3)\n",
      "7mv5E2yb2yVQU34OiQ1vqv (747, 3)\n",
      "5Sg6efUjypR4m6p9eYBXpm (379, 3)\n",
      "3iydyD9rAb1f6rmvmgpwS4 (447, 3)\n",
      "4pFaG2QLnDr95gqDQFEWoh (390, 3)\n",
      "4DUIcbw3EZpeYUC2mcxV0D (229, 3)\n",
      "0ZGQ63222rqX5TD5ZrMmcN (601, 3)\n",
      "2Bp5vd9GAmEpZzjEtGQBFD (53, 3)\n",
      "1VBbCB6ja5pPdU2wrBy27N (560, 3)\n",
      "3DR5Qa40Mc17AiBYfmC29U (223, 3)\n",
      "53DrbE5nPJskpPT0PtOi9O (75, 3)\n",
      "3p9FLEH5V5sCGHhGubaYZc (187, 3)\n",
      "28IWswylk2FvkebOehoCkL (351, 3)\n",
      "5ts4p0QlyePWCgIB2W1wLf (234, 3)\n"
     ]
    }
   ],
   "source": [
    "# Make dataset for predition of each og the annotated transcripts\n",
    "for episode in annotated_transcripts_plain.episode_id:\n",
    "    transcript_to_use = annotated_transcripts_plain[annotated_transcripts_plain.episode_id == episode].copy()\n",
    "    prediction_data = sentence_chunk_transcript(transcript_to_use, name_variable='transcript', chunk_size=1)\n",
    "    prediction_data.to_csv(f'../Thesis/annotated_transcripts/{episode}_data_for_prediction.csv')\n",
    "    \n",
    "    print(episode, prediction_data.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
