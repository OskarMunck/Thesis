{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def tokenize_split(text):\n",
    "    \"\"\"simple tokeniser\"\"\"\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def word_chunk_transcript(transcripts, name_variable='transcript', chunk_size=500):\n",
    "    \"\"\" transcripts: must have two columns - episode_id and tokenised_transcript\n",
    "        chunk_size: number of tokens in chunk \n",
    "    \"\"\"\n",
    "    transcripts[\"tokenised_transcript\"] = transcripts[name_variable].apply(lambda x: tokenize_split(x))\n",
    "\n",
    "    episode_ls = []\n",
    "    transcript_ls = []\n",
    "    words_enum_ls = [] \n",
    "    for index, row in transcripts.iterrows():\n",
    "        for i in range(0, len(row[\"tokenised_transcript\"]), chunk_size):\n",
    "            episode_ls.append(row[\"episode_id\"])\n",
    "            transcript_ls.append(\" \".join(row[\"tokenised_transcript\"][i:i+chunk_size]))\n",
    "            words_enum_ls.append(f\"{i+1} - {i+chunk_size}\")\n",
    "    word_chunked_df = pd.DataFrame(data = {'episode_id': episode_ls, 'transcript_subset':transcript_ls, 'words_enumerated':words_enum_ls})\n",
    "    return word_chunked_df\n",
    "\n",
    "\n",
    "def sentence_chunk_transcript(transcripts, name_variable='transcript', chunk_size=1):\n",
    "    \"\"\" transcripts: must have two columns - episode_id and transcript\n",
    "        chunk_size: number of sentences in chunk \n",
    "    \"\"\"\n",
    "    transcripts[\"sentence_token\"] = transcripts[name_variable].apply(lambda x: sent_tokenize(x, language='english'))\n",
    "    episode_ls = []\n",
    "    transcript_ls = []\n",
    "    sent_enum_ls = []\n",
    "    for index, row in transcripts.iterrows():\n",
    "        for i in range(0, len(row[\"sentence_token\"]), chunk_size):\n",
    "            episode_ls.append(row[\"episode_id\"])\n",
    "            transcript_ls.append(\" \".join(row[\"sentence_token\"][i:i+chunk_size]))\n",
    "            sent_enum_ls.append(i+1)\n",
    "    sentence_chunked_df = pd.DataFrame(data = {'episode_id': episode_ls, 'transcript_subset':transcript_ls, 'sentence_enumerated':sent_enum_ls})\n",
    "    return sentence_chunked_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make chunked dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11821, 17)\n"
     ]
    }
   ],
   "source": [
    "transcripts_sample = pd.read_csv('sports_transcripts.csv.gz', compression='gzip')\n",
    "print(transcripts_sample.shape)\n",
    "# transcripts_sample.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract columns for conversion\n",
    "cols_subset = transcripts_sample.loc[: ,[\"episode_id\", \"transcript\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(368835, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>transcript_subset</th>\n",
       "      <th>words_enumerated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41JbXYp7c2uuJoFB4TcQtD</td>\n",
       "      <td>Hello and welcome to the law review podcast. M...</td>\n",
       "      <td>0 - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41JbXYp7c2uuJoFB4TcQtD</td>\n",
       "      <td>as well as several other changes, so we'll get...</td>\n",
       "      <td>256 - 512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               episode_id                                  transcript_subset  \\\n",
       "0  41JbXYp7c2uuJoFB4TcQtD  Hello and welcome to the law review podcast. M...   \n",
       "1  41JbXYp7c2uuJoFB4TcQtD  as well as several other changes, so we'll get...   \n",
       "\n",
       "  words_enumerated  \n",
       "0          0 - 256  \n",
       "1        256 - 512  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sports_256 = word_chunk_transcript(cols_subset, name_variable='transcript', chunk_size=256)\n",
    "\n",
    "print(sports_256.shape)\n",
    "sports_256.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_256.to_csv('sports_word_256.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5460190, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>transcript_subset</th>\n",
       "      <th>sentence_enumerated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41JbXYp7c2uuJoFB4TcQtD</td>\n",
       "      <td>Hello and welcome to the law review podcast.</td>\n",
       "      <td>0 - 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41JbXYp7c2uuJoFB4TcQtD</td>\n",
       "      <td>My name is Nathan Church Droid by my partner c...</td>\n",
       "      <td>1 - 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               episode_id                                  transcript_subset  \\\n",
       "0  41JbXYp7c2uuJoFB4TcQtD       Hello and welcome to the law review podcast.   \n",
       "1  41JbXYp7c2uuJoFB4TcQtD  My name is Nathan Church Droid by my partner c...   \n",
       "\n",
       "  sentence_enumerated  \n",
       "0               0 - 1  \n",
       "1               1 - 2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sport_sent_1 = sentence_chunk_transcript(cols_subset, name_variable='transcript', chunk_size=1)\n",
    "print(sport_sent_1.shape)\n",
    "sport_sent_1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sport_sent_1 = sport_sent_1.sample(500000, random_state=42)\n",
    "sport_sent_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sport_sent_1.to_csv('sport_sent_1.csv.gz', compression='gzip')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>show_id</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>transcript</th>\n",
       "      <th>avg_confidence</th>\n",
       "      <th>char_count</th>\n",
       "      <th>show_name</th>\n",
       "      <th>show_description</th>\n",
       "      <th>publisher</th>\n",
       "      <th>language</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>episode_description</th>\n",
       "      <th>duration</th>\n",
       "      <th>show_id_trans</th>\n",
       "      <th>category</th>\n",
       "      <th>pubdate</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>3701</td>\n",
       "      <td>show_2z1LtWVnflRUONFAo0FADb</td>\n",
       "      <td>6preEOWrgR9eRr938upFgv</td>\n",
       "      <td>I were back. It's another Carolina podcast. Ha...</td>\n",
       "      <td>0.820242</td>\n",
       "      <td>60218</td>\n",
       "      <td>Gamecock Central Podcast Network</td>\n",
       "      <td>The Gamecock Central Podcast Network brings yo...</td>\n",
       "      <td>Gamecock Central Podcasts</td>\n",
       "      <td>['en']</td>\n",
       "      <td>Another Carolina Podcast: Early Enrollees and ...</td>\n",
       "      <td>Wes Mitchell and Chris Clark join host Pearson...</td>\n",
       "      <td>55.61905</td>\n",
       "      <td>show_2z1LtWVnflRUONFAo0FADb</td>\n",
       "      <td>Sports</td>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>11390</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     Unnamed: 0                      show_id              episode_id  \\\n",
       "384        3701  show_2z1LtWVnflRUONFAo0FADb  6preEOWrgR9eRr938upFgv   \n",
       "\n",
       "                                            transcript  avg_confidence  \\\n",
       "384  I were back. It's another Carolina podcast. Ha...        0.820242   \n",
       "\n",
       "     char_count                         show_name  \\\n",
       "384       60218  Gamecock Central Podcast Network   \n",
       "\n",
       "                                      show_description  \\\n",
       "384  The Gamecock Central Podcast Network brings yo...   \n",
       "\n",
       "                     publisher language  \\\n",
       "384  Gamecock Central Podcasts   ['en']   \n",
       "\n",
       "                                          episode_name  \\\n",
       "384  Another Carolina Podcast: Early Enrollees and ...   \n",
       "\n",
       "                                   episode_description  duration  \\\n",
       "384  Wes Mitchell and Chris Clark join host Pearson...  55.61905   \n",
       "\n",
       "                   show_id_trans category     pubdate  word_count  \n",
       "384  show_2z1LtWVnflRUONFAo0FADb   Sports  2020-01-08       11390  "
      ]
     },
     "execution_count": 134,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv('sports_transcripts.csv.gz', compression='gzip')\n",
    "data_to_use = data[data.episode_id == '6preEOWrgR9eRr938upFgv'].copy()\n",
    "data_to_use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(599, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>transcript_subset</th>\n",
       "      <th>sentence_enumerated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>6preEOWrgR9eRr938upFgv</td>\n",
       "      <td>I were back.</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6preEOWrgR9eRr938upFgv</td>\n",
       "      <td>It's another Carolina podcast.</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>6preEOWrgR9eRr938upFgv</td>\n",
       "      <td>Happy New Year.</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>6preEOWrgR9eRr938upFgv</td>\n",
       "      <td>Everybody first another Carolina podcast with ...</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6preEOWrgR9eRr938upFgv</td>\n",
       "      <td>First time.</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               episode_id                                  transcript_subset  \\\n",
       "0  6preEOWrgR9eRr938upFgv                                       I were back.   \n",
       "1  6preEOWrgR9eRr938upFgv                     It's another Carolina podcast.   \n",
       "2  6preEOWrgR9eRr938upFgv                                    Happy New Year.   \n",
       "3  6preEOWrgR9eRr938upFgv  Everybody first another Carolina podcast with ...   \n",
       "4  6preEOWrgR9eRr938upFgv                                        First time.   \n",
       "\n",
       "   sentence_enumerated  \n",
       "0                    1  \n",
       "1                    2  \n",
       "2                    3  \n",
       "3                    4  \n",
       "4                    5  "
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_data = sentence_chunk_transcript(data_to_use, name_variable='transcript', chunk_size=1)\n",
    "prediction_data.to_csv('prediction_data.csv')\n",
    "print(prediction_data.shape)\n",
    "prediction_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>transcript_subset</th>\n",
       "      <th>sentence_enumerated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6preEOWrgR9eRr938upFgv</td>\n",
       "      <td>I would still even if Clemson offers I still t...</td>\n",
       "      <td>146</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6preEOWrgR9eRr938upFgv</td>\n",
       "      <td>And you know Chris.</td>\n",
       "      <td>147</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6preEOWrgR9eRr938upFgv</td>\n",
       "      <td>I think it actually the other side of that is ...</td>\n",
       "      <td>148</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6preEOWrgR9eRr938upFgv</td>\n",
       "      <td>We've seen him blow up he went from you know, ...</td>\n",
       "      <td>149</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>6preEOWrgR9eRr938upFgv</td>\n",
       "      <td>So if Clemson does offer the fact that they're...</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150</th>\n",
       "      <td>6preEOWrgR9eRr938upFgv</td>\n",
       "      <td>But yeah, so the fact that those guys want thi...</td>\n",
       "      <td>151</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>151</th>\n",
       "      <td>6preEOWrgR9eRr938upFgv</td>\n",
       "      <td>Yeah for sure.</td>\n",
       "      <td>152</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>152</th>\n",
       "      <td>6preEOWrgR9eRr938upFgv</td>\n",
       "      <td>I mean And part of it.</td>\n",
       "      <td>153</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>153</th>\n",
       "      <td>6preEOWrgR9eRr938upFgv</td>\n",
       "      <td>I think one of part of it is you get a little ...</td>\n",
       "      <td>154</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>154</th>\n",
       "      <td>6preEOWrgR9eRr938upFgv</td>\n",
       "      <td>They were one of the first Rider the first to ...</td>\n",
       "      <td>155</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>155</th>\n",
       "      <td>6preEOWrgR9eRr938upFgv</td>\n",
       "      <td>There are 101 one at one of the major was actu...</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>156</th>\n",
       "      <td>6preEOWrgR9eRr938upFgv</td>\n",
       "      <td>Yeah.</td>\n",
       "      <td>157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>157</th>\n",
       "      <td>6preEOWrgR9eRr938upFgv</td>\n",
       "      <td>He'd have the be right down the road.</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>158</th>\n",
       "      <td>6preEOWrgR9eRr938upFgv</td>\n",
       "      <td>Yeah easy, but but yeah, so I think you look a...</td>\n",
       "      <td>159</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>159</th>\n",
       "      <td>6preEOWrgR9eRr938upFgv</td>\n",
       "      <td>Also that Bobby Bentley had built their before...</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 episode_id  \\\n",
       "145  6preEOWrgR9eRr938upFgv   \n",
       "146  6preEOWrgR9eRr938upFgv   \n",
       "147  6preEOWrgR9eRr938upFgv   \n",
       "148  6preEOWrgR9eRr938upFgv   \n",
       "149  6preEOWrgR9eRr938upFgv   \n",
       "150  6preEOWrgR9eRr938upFgv   \n",
       "151  6preEOWrgR9eRr938upFgv   \n",
       "152  6preEOWrgR9eRr938upFgv   \n",
       "153  6preEOWrgR9eRr938upFgv   \n",
       "154  6preEOWrgR9eRr938upFgv   \n",
       "155  6preEOWrgR9eRr938upFgv   \n",
       "156  6preEOWrgR9eRr938upFgv   \n",
       "157  6preEOWrgR9eRr938upFgv   \n",
       "158  6preEOWrgR9eRr938upFgv   \n",
       "159  6preEOWrgR9eRr938upFgv   \n",
       "\n",
       "                                     transcript_subset  sentence_enumerated  \n",
       "145  I would still even if Clemson offers I still t...                  146  \n",
       "146                                And you know Chris.                  147  \n",
       "147  I think it actually the other side of that is ...                  148  \n",
       "148  We've seen him blow up he went from you know, ...                  149  \n",
       "149  So if Clemson does offer the fact that they're...                  150  \n",
       "150  But yeah, so the fact that those guys want thi...                  151  \n",
       "151                                     Yeah for sure.                  152  \n",
       "152                             I mean And part of it.                  153  \n",
       "153  I think one of part of it is you get a little ...                  154  \n",
       "154  They were one of the first Rider the first to ...                  155  \n",
       "155  There are 101 one at one of the major was actu...                  156  \n",
       "156                                              Yeah.                  157  \n",
       "157              He'd have the be right down the road.                  158  \n",
       "158  Yeah easy, but but yeah, so I think you look a...                  159  \n",
       "159  Also that Bobby Bentley had built their before...                  160  "
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_data.iloc[145:160,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract transcripts for annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word count of annotated transcripts: 8023\n"
     ]
    }
   ],
   "source": [
    "sports_trans = pd.read_csv('sports_transcripts.csv.gz', compression='gzip', usecols=[2,3,16])\n",
    "sports_trans = sports_trans.sample(200)\n",
    "print(f'Average word count of annotated transcripts: {sports_trans.word_count.mean():.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write transcripts to files\n",
    "import os.path\n",
    "\n",
    "for i in range(len(sports_trans.index)):\n",
    "    save_path = '../Thesis/annotated_transcripts/'\n",
    "    name_of_file = f'transcript{i+1}_{sports_trans.iloc[i,0]}'\n",
    "    complete_path = os.path.join(save_path, name_of_file+\".txt\")\n",
    "\n",
    "    text_file = open(complete_path, \"w\")\n",
    "    text_file.write(sports_trans.iloc[i,1])\n",
    "    text_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read back and output list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "directory = '../Thesis/annotated_transcripts'\n",
    "\n",
    "annotated_ls = []\n",
    "episode_ls = []\n",
    "for dirpath, dirnames, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        episode_ls.append(file)\n",
    "        path = os.path.join(dirpath, file)\n",
    "        with open(path) as f: \n",
    "            lines = f.readlines()\n",
    "            annotated_ls.append(''.join(lines))\n",
    "\n",
    "annotated_df = pd.DataFrame({'transcript': annotated_ls, 'episode_id': episode_ls})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_df = annotated_df[annotated_df.episode_id == 'transcript1_6preEOWrgR9eRr938upFgv.txt'].copy()\n",
    "df_per_transcript = sentence_chunk_transcript(annotated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transcript1_6preEOWrgR9eRr938upFgv.txt</td>\n",
       "      <td>[4, 39, 71]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transcript2_7mv5E2yb2yVQU34OiQ1vqv.txt</td>\n",
       "      <td>[37, 67]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               episode_id   annotation\n",
       "0  transcript1_6preEOWrgR9eRr938upFgv.txt  [4, 39, 71]\n",
       "0  transcript2_7mv5E2yb2yVQU34OiQ1vqv.txt     [37, 67]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_id = df_per_transcript.episode_id.unique()\n",
    "\n",
    "annotation_values = pd.DataFrame(columns = ['episode_id', 'annotation'])\n",
    "\n",
    "for id in unique_id:\n",
    "    find_annotation = df_per_transcript[df_per_transcript.episode_id == id].copy()\n",
    "    annotated_index_ls = []\n",
    "    for row in find_annotation.itertuples():\n",
    "        if row[2].startswith('@@'):\n",
    "            annotated_index_ls.append(row[3])\n",
    "    if len(annotated_index_ls) != 0:\n",
    "        temp_df = pd.DataFrame({'episode_id': id, 'annotation':[annotated_index_ls]})\n",
    "        annotation_values = pd.concat([annotation_values, temp_df], axis=0)\n",
    "\n",
    "annotation_values.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
