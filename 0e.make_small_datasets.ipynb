{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def tokenize_split(text):\n",
    "    \"\"\"simple tokeniser\"\"\"\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "def word_chunk_transcript(transcripts, name_variable='transcript', chunk_size=500):\n",
    "    \"\"\" transcripts: must have two columns - episode_id and tokenised_transcript\n",
    "        chunk_size: number of tokens in chunk \n",
    "    \"\"\"\n",
    "    transcripts[\"tokenised_transcript\"] = transcripts[name_variable].apply(lambda x: tokenize_split(x))\n",
    "\n",
    "    episode_ls = []\n",
    "    transcript_ls = []\n",
    "    words_enum_ls = [] \n",
    "    for index, row in transcripts.iterrows():\n",
    "        for i in range(0, len(row[\"tokenised_transcript\"]), chunk_size):\n",
    "            episode_ls.append(row[\"episode_id\"])\n",
    "            transcript_ls.append(\" \".join(row[\"tokenised_transcript\"][i:i+chunk_size]))\n",
    "            words_enum_ls.append(f\"{i+1} - {i+chunk_size}\")\n",
    "    word_chunked_df = pd.DataFrame(data = {'episode_id': episode_ls, 'transcript_subset':transcript_ls, 'words_enumerated':words_enum_ls})\n",
    "    return word_chunked_df\n",
    "\n",
    "\n",
    "def sentence_chunk_transcript(transcripts, name_variable='transcript', chunk_size=1):\n",
    "    \"\"\" transcripts: must have two columns - episode_id and transcript\n",
    "        chunk_size: number of sentences in chunk \n",
    "    \"\"\"\n",
    "    transcripts[\"sentence_token\"] = transcripts[name_variable].apply(lambda x: sent_tokenize(x, language='english'))\n",
    "    episode_ls = []\n",
    "    transcript_ls = []\n",
    "    sent_enum_ls = []\n",
    "    for index, row in transcripts.iterrows():\n",
    "        for i in range(0, len(row[\"sentence_token\"]), chunk_size):\n",
    "            episode_ls.append(row[\"episode_id\"])\n",
    "            transcript_ls.append(\" \".join(row[\"sentence_token\"][i:i+chunk_size]))\n",
    "            sent_enum_ls.append(i+1)\n",
    "    sentence_chunked_df = pd.DataFrame(data = {'episode_id': episode_ls, 'transcript_subset':transcript_ls, 'sentence_enumerated':sent_enum_ls})\n",
    "    return sentence_chunked_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make chunked dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(11821, 17)\n"
     ]
    }
   ],
   "source": [
    "transcripts_sample = pd.read_csv('sports_transcripts.csv.gz', compression='gzip')\n",
    "print(transcripts_sample.shape)\n",
    "# transcripts_sample.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract columns for conversion\n",
    "cols_subset = transcripts_sample.loc[: ,[\"episode_id\", \"transcript\"]]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create new datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(368835, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>transcript_subset</th>\n",
       "      <th>words_enumerated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41JbXYp7c2uuJoFB4TcQtD</td>\n",
       "      <td>Hello and welcome to the law review podcast. M...</td>\n",
       "      <td>0 - 256</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41JbXYp7c2uuJoFB4TcQtD</td>\n",
       "      <td>as well as several other changes, so we'll get...</td>\n",
       "      <td>256 - 512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               episode_id                                  transcript_subset  \\\n",
       "0  41JbXYp7c2uuJoFB4TcQtD  Hello and welcome to the law review podcast. M...   \n",
       "1  41JbXYp7c2uuJoFB4TcQtD  as well as several other changes, so we'll get...   \n",
       "\n",
       "  words_enumerated  \n",
       "0          0 - 256  \n",
       "1        256 - 512  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sports_256 = word_chunk_transcript(cols_subset, name_variable='transcript', chunk_size=256)\n",
    "\n",
    "print(sports_256.shape)\n",
    "sports_256.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "sports_256.to_csv('sports_word_256.csv.gz', compression='gzip')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5460190, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>transcript_subset</th>\n",
       "      <th>sentence_enumerated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>41JbXYp7c2uuJoFB4TcQtD</td>\n",
       "      <td>Hello and welcome to the law review podcast.</td>\n",
       "      <td>0 - 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>41JbXYp7c2uuJoFB4TcQtD</td>\n",
       "      <td>My name is Nathan Church Droid by my partner c...</td>\n",
       "      <td>1 - 2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               episode_id                                  transcript_subset  \\\n",
       "0  41JbXYp7c2uuJoFB4TcQtD       Hello and welcome to the law review podcast.   \n",
       "1  41JbXYp7c2uuJoFB4TcQtD  My name is Nathan Church Droid by my partner c...   \n",
       "\n",
       "  sentence_enumerated  \n",
       "0               0 - 1  \n",
       "1               1 - 2  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sport_sent_1 = sentence_chunk_transcript(cols_subset, name_variable='transcript', chunk_size=1)\n",
    "print(sport_sent_1.shape)\n",
    "sport_sent_1.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000, 3)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sport_sent_1 = sport_sent_1.sample(500000, random_state=42)\n",
    "sport_sent_1.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "sport_sent_1.to_csv('sport_sent_1.csv.gz', compression='gzip')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pipeline for segmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>show_id</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>transcript</th>\n",
       "      <th>avg_confidence</th>\n",
       "      <th>char_count</th>\n",
       "      <th>show_name</th>\n",
       "      <th>show_description</th>\n",
       "      <th>publisher</th>\n",
       "      <th>language</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>episode_description</th>\n",
       "      <th>duration</th>\n",
       "      <th>show_id_trans</th>\n",
       "      <th>category</th>\n",
       "      <th>pubdate</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>31</td>\n",
       "      <td>show_2UpbOw7HZDVpHaieaeswzj</td>\n",
       "      <td>1rv5FRQHZGm2VZbyj0QZtm</td>\n",
       "      <td>Hey guys are in take a quick break here to tal...</td>\n",
       "      <td>0.846479</td>\n",
       "      <td>59985</td>\n",
       "      <td>No Extra Points - An AAF Podumentary</td>\n",
       "      <td>When the Alliance of American Football debuted...</td>\n",
       "      <td>William Renken</td>\n",
       "      <td>['en']</td>\n",
       "      <td>No Extra Points - An AAF Podumentary</td>\n",
       "      <td>In March 2018, Charlie Ebersol announced to th...</td>\n",
       "      <td>67.653667</td>\n",
       "      <td>show_2UpbOw7HZDVpHaieaeswzj</td>\n",
       "      <td>Sports</td>\n",
       "      <td>2019-06-17</td>\n",
       "      <td>11233</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                      show_id              episode_id  \\\n",
       "1          31  show_2UpbOw7HZDVpHaieaeswzj  1rv5FRQHZGm2VZbyj0QZtm   \n",
       "\n",
       "                                          transcript  avg_confidence  \\\n",
       "1  Hey guys are in take a quick break here to tal...        0.846479   \n",
       "\n",
       "   char_count                             show_name  \\\n",
       "1       59985  No Extra Points - An AAF Podumentary   \n",
       "\n",
       "                                    show_description       publisher language  \\\n",
       "1  When the Alliance of American Football debuted...  William Renken   ['en']   \n",
       "\n",
       "                           episode_name  \\\n",
       "1  No Extra Points - An AAF Podumentary   \n",
       "\n",
       "                                 episode_description   duration  \\\n",
       "1  In March 2018, Charlie Ebersol announced to th...  67.653667   \n",
       "\n",
       "                 show_id_trans category     pubdate  word_count  \n",
       "1  show_2UpbOw7HZDVpHaieaeswzj   Sports  2019-06-17       11233  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load dataset\n",
    "data = pd.read_csv('sports_transcripts.csv.gz', compression='gzip', nrows=5)\n",
    "data_to_use = data.iloc[[1]].copy()\n",
    "data_to_use.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(468, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>transcript_subset</th>\n",
       "      <th>sentence_enumerated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1rv5FRQHZGm2VZbyj0QZtm</td>\n",
       "      <td>Hey guys are in take a quick break here to tal...</td>\n",
       "      <td>1 - 1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1rv5FRQHZGm2VZbyj0QZtm</td>\n",
       "      <td>What's holding you up?</td>\n",
       "      <td>2 - 2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1rv5FRQHZGm2VZbyj0QZtm</td>\n",
       "      <td>Because it's free that's always the biggest th...</td>\n",
       "      <td>3 - 3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1rv5FRQHZGm2VZbyj0QZtm</td>\n",
       "      <td>You need to make your podcast go.</td>\n",
       "      <td>4 - 4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1rv5FRQHZGm2VZbyj0QZtm</td>\n",
       "      <td>You don't have to worry about necessarily owni...</td>\n",
       "      <td>5 - 5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               episode_id                                  transcript_subset  \\\n",
       "0  1rv5FRQHZGm2VZbyj0QZtm  Hey guys are in take a quick break here to tal...   \n",
       "1  1rv5FRQHZGm2VZbyj0QZtm                             What's holding you up?   \n",
       "2  1rv5FRQHZGm2VZbyj0QZtm  Because it's free that's always the biggest th...   \n",
       "3  1rv5FRQHZGm2VZbyj0QZtm                  You need to make your podcast go.   \n",
       "4  1rv5FRQHZGm2VZbyj0QZtm  You don't have to worry about necessarily owni...   \n",
       "\n",
       "  sentence_enumerated  \n",
       "0               1 - 1  \n",
       "1               2 - 2  \n",
       "2               3 - 3  \n",
       "3               4 - 4  \n",
       "4               5 - 5  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction_data = sentence_chunk_transcript(data_to_use, name_variable='transcript', chunk_size=1)\n",
    "prediction_data.to_csv('prediction_data.csv')\n",
    "print(prediction_data.shape)\n",
    "prediction_data.head()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract transcripts for annotation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average word count of annotated transcripts: 8023\n"
     ]
    }
   ],
   "source": [
    "sports_trans = pd.read_csv('sports_transcripts.csv.gz', compression='gzip', usecols=[2,3,16])\n",
    "sports_trans = sports_trans.sample(200)\n",
    "print(f'Average word count of annotated transcripts: {sports_trans.word_count.mean():.0f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write transcripts to files\n",
    "import os.path\n",
    "\n",
    "for i in range(len(sports_trans.index)):\n",
    "    save_path = '../Thesis/annotated_transcripts/'\n",
    "    name_of_file = f'transcript{i+1}_{sports_trans.iloc[i,0]}'\n",
    "    complete_path = os.path.join(save_path, name_of_file+\".txt\")\n",
    "\n",
    "    text_file = open(complete_path, \"w\")\n",
    "    text_file.write(sports_trans.iloc[i,1])\n",
    "    text_file.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read back and output list "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "directory = '../Thesis/annotated_transcripts'\n",
    "\n",
    "annotated_ls = []\n",
    "episode_ls = []\n",
    "for dirpath, dirnames, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        episode_ls.append(file)\n",
    "        path = os.path.join(dirpath, file)\n",
    "        with open(path) as f: \n",
    "            lines = f.readlines()\n",
    "            annotated_ls.append(''.join(lines))\n",
    "\n",
    "annotated_df = pd.DataFrame({'transcript': annotated_ls, 'episode_id': episode_ls})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [],
   "source": [
    "# temp_df = annotated_df[annotated_df.episode_id == 'transcript1_6preEOWrgR9eRr938upFgv.txt'].copy()\n",
    "df_per_transcript = sentence_chunk_transcript(annotated_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>annotation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transcript1_6preEOWrgR9eRr938upFgv.txt</td>\n",
       "      <td>[4, 39, 71]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>transcript2_7mv5E2yb2yVQU34OiQ1vqv.txt</td>\n",
       "      <td>[37, 67]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                               episode_id   annotation\n",
       "0  transcript1_6preEOWrgR9eRr938upFgv.txt  [4, 39, 71]\n",
       "0  transcript2_7mv5E2yb2yVQU34OiQ1vqv.txt     [37, 67]"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_id = df_per_transcript.episode_id.unique()\n",
    "\n",
    "annotation_values = pd.DataFrame(columns = ['episode_id', 'annotation'])\n",
    "\n",
    "for id in unique_id:\n",
    "    find_annotation = df_per_transcript[df_per_transcript.episode_id == id].copy()\n",
    "    annotated_index_ls = []\n",
    "    for row in find_annotation.itertuples():\n",
    "        if row[2].startswith('@@'):\n",
    "            annotated_index_ls.append(row[3])\n",
    "    if len(annotated_index_ls) != 0:\n",
    "        temp_df = pd.DataFrame({'episode_id': id, 'annotation':[annotated_index_ls]})\n",
    "        annotation_values = pd.concat([annotation_values, temp_df], axis=0)\n",
    "\n",
    "annotation_values.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
