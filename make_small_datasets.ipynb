{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def tokenize_split(text):\n",
    "    \"\"\"simple tokeniser\"\"\"\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "# Functions for chunking transcripts on either words or sentences\n",
    "\n",
    "def word_chunk_transcript(transcripts, chunk_size=500):\n",
    "    \"\"\" transcripts: must have two columns - episode_id and tokenised_transcript\n",
    "        chunk_size: number of tokens in chunk \n",
    "    \"\"\"\n",
    "    transcripts[\"tokenised_transcript\"] = transcripts.transcript.apply(lambda x: tokenize_split(x))\n",
    "\n",
    "    episode_ls = []\n",
    "    transcript_ls = []\n",
    "    words_enum_ls = [] \n",
    "    for index, row in transcripts.iterrows():\n",
    "        for i in range(0, len(row[\"tokenised_transcript\"]), chunk_size):\n",
    "            episode_ls.append(row[\"episode_id\"])\n",
    "            transcript_ls.append(\" \".join(row[\"tokenised_transcript\"][i:i+chunk_size]))\n",
    "            words_enum_ls.append(f\"{i} - {i+chunk_size}\")\n",
    "    word_chunked_df = pd.DataFrame(data = {'episode_id': episode_ls, 'transcript_subset':transcript_ls, 'words_enumerated':words_enum_ls})\n",
    "    return word_chunked_df\n",
    "\n",
    "\n",
    "def sentence_chunk_transcript(transcripts, chunk_size=20):\n",
    "    \"\"\" transcripts: must have two columns - episode_id and transcript\n",
    "        chunk_size: number of sentences in chunk \n",
    "    \"\"\"\n",
    "    transcripts[\"sentence_token\"] = transcripts.transcript.apply(lambda x: sent_tokenize(x, language='english'))\n",
    "    episode_ls = []\n",
    "    transcript_ls = []\n",
    "    sent_enum_ls = []\n",
    "    for index, row in transcripts.iterrows():\n",
    "        for i in range(0, len(row[\"sentence_token\"]), chunk_size):\n",
    "            episode_ls.append(row[\"episode_id\"])\n",
    "            transcript_ls.append(\" \".join(row[\"sentence_token\"][i:i+chunk_size]))\n",
    "            sent_enum_ls.append(f\"{i} - {i+chunk_size}\")\n",
    "    sentence_chunked_df = pd.DataFrame(data = {'episode_id': episode_ls, 'transcript_subset':transcript_ls, 'sentence_enumerated':sent_enum_ls})\n",
    "    return sentence_chunked_df\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make chunked dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 14)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>show_id</th>\n",
       "      <th>episode_id</th>\n",
       "      <th>transcript</th>\n",
       "      <th>avg_confidence</th>\n",
       "      <th>char_count</th>\n",
       "      <th>show_name</th>\n",
       "      <th>show_description</th>\n",
       "      <th>publisher</th>\n",
       "      <th>language</th>\n",
       "      <th>episode_name</th>\n",
       "      <th>episode_description</th>\n",
       "      <th>duration</th>\n",
       "      <th>show_id_trans</th>\n",
       "      <th>word_count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>show_1jUyEaMpjfOYjcwPCdEaec</td>\n",
       "      <td>35xHkzb4wNPpqipwjAkmDI</td>\n",
       "      <td>Hello and welcome along to the property Academ...</td>\n",
       "      <td>0.820831</td>\n",
       "      <td>13516</td>\n",
       "      <td>The Property Academy Podcast</td>\n",
       "      <td>The Property Academy Podcast is a daily show t...</td>\n",
       "      <td>Opes Partners</td>\n",
       "      <td>['en']</td>\n",
       "      <td>The Additional Costs Associated with Airbnb | ...</td>\n",
       "      <td>In this episode, we discuss the additional cos...</td>\n",
       "      <td>13.906433</td>\n",
       "      <td>show_1jUyEaMpjfOYjcwPCdEaec</td>\n",
       "      <td>2507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>show_0u6NNu3ZyZHyn888FD3WdE</td>\n",
       "      <td>5jYwyaLp8PDnQondFv77kC</td>\n",
       "      <td>Good morning, everyone. This is Trinity here a...</td>\n",
       "      <td>0.818924</td>\n",
       "      <td>25849</td>\n",
       "      <td>TheProdcast</td>\n",
       "      <td>The Prodcast - all about the stars behind stel...</td>\n",
       "      <td>TheProdcast</td>\n",
       "      <td>['en']</td>\n",
       "      <td>Episode 6: How tech meets travel—redefining va...</td>\n",
       "      <td>Travel—the very word is enough to instil energ...</td>\n",
       "      <td>25.466783</td>\n",
       "      <td>show_0u6NNu3ZyZHyn888FD3WdE</td>\n",
       "      <td>4834</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>show_6KLpvCAxrVzbsnBnRs8O4I</td>\n",
       "      <td>12UFlPPdjCBpFibZQnnwLe</td>\n",
       "      <td>Hey guys, it's Peter fry and welcome to the li...</td>\n",
       "      <td>0.805737</td>\n",
       "      <td>4894</td>\n",
       "      <td>Living with Hope Podcast with Peter Frey</td>\n",
       "      <td>Welcome to the Living with Hope podcast with P...</td>\n",
       "      <td>Peter Frey</td>\n",
       "      <td>['en']</td>\n",
       "      <td>IMMEASURABLY MORE | Ephesians 3:14-21 | Living...</td>\n",
       "      <td>Paul's prayer for the Ephesians guides us toda...</td>\n",
       "      <td>6.436567</td>\n",
       "      <td>show_6KLpvCAxrVzbsnBnRs8O4I</td>\n",
       "      <td>998</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>show_1HvChDzJwUYPX4YU7JJ5Aj</td>\n",
       "      <td>3NfJNHjBIW6IMsg8gGN9Th</td>\n",
       "      <td>Hey afterbuzzers, before we move on to your ne...</td>\n",
       "      <td>0.819311</td>\n",
       "      <td>20993</td>\n",
       "      <td>The Good Place After Show Podcast</td>\n",
       "      <td>If philosophical discussions on life and the a...</td>\n",
       "      <td>AfterBuzz TV</td>\n",
       "      <td>['en-US']</td>\n",
       "      <td>\"The Funeral to End All Funerals\" Season 4 Epi...</td>\n",
       "      <td>Good Janet and Bad Janet unite?! And the Judge...</td>\n",
       "      <td>23.081800</td>\n",
       "      <td>show_1HvChDzJwUYPX4YU7JJ5Aj</td>\n",
       "      <td>4080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>show_6rUa8ruUHI2kl7DjyzxBdw</td>\n",
       "      <td>36uhfvspHI1lsjsdfJ0xlz</td>\n",
       "      <td>Have you ever wondered what it's like to be pr...</td>\n",
       "      <td>0.860390</td>\n",
       "      <td>640</td>\n",
       "      <td>30 and Pregnant</td>\n",
       "      <td>One woman’s journey through pregnancy, week by...</td>\n",
       "      <td>Abby</td>\n",
       "      <td>['en']</td>\n",
       "      <td>30 and Pregnant  (Trailer)</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.770133</td>\n",
       "      <td>show_6rUa8ruUHI2kl7DjyzxBdw</td>\n",
       "      <td>120</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                       show_id              episode_id  \\\n",
       "0  show_1jUyEaMpjfOYjcwPCdEaec  35xHkzb4wNPpqipwjAkmDI   \n",
       "1  show_0u6NNu3ZyZHyn888FD3WdE  5jYwyaLp8PDnQondFv77kC   \n",
       "2  show_6KLpvCAxrVzbsnBnRs8O4I  12UFlPPdjCBpFibZQnnwLe   \n",
       "3  show_1HvChDzJwUYPX4YU7JJ5Aj  3NfJNHjBIW6IMsg8gGN9Th   \n",
       "4  show_6rUa8ruUHI2kl7DjyzxBdw  36uhfvspHI1lsjsdfJ0xlz   \n",
       "\n",
       "                                          transcript  avg_confidence  \\\n",
       "0  Hello and welcome along to the property Academ...        0.820831   \n",
       "1  Good morning, everyone. This is Trinity here a...        0.818924   \n",
       "2  Hey guys, it's Peter fry and welcome to the li...        0.805737   \n",
       "3  Hey afterbuzzers, before we move on to your ne...        0.819311   \n",
       "4  Have you ever wondered what it's like to be pr...        0.860390   \n",
       "\n",
       "   char_count                                 show_name  \\\n",
       "0       13516              The Property Academy Podcast   \n",
       "1       25849                               TheProdcast   \n",
       "2        4894  Living with Hope Podcast with Peter Frey   \n",
       "3       20993         The Good Place After Show Podcast   \n",
       "4         640                          30 and Pregnant    \n",
       "\n",
       "                                    show_description      publisher  \\\n",
       "0  The Property Academy Podcast is a daily show t...  Opes Partners   \n",
       "1  The Prodcast - all about the stars behind stel...    TheProdcast   \n",
       "2  Welcome to the Living with Hope podcast with P...     Peter Frey   \n",
       "3  If philosophical discussions on life and the a...   AfterBuzz TV   \n",
       "4  One woman’s journey through pregnancy, week by...           Abby   \n",
       "\n",
       "    language                                       episode_name  \\\n",
       "0     ['en']  The Additional Costs Associated with Airbnb | ...   \n",
       "1     ['en']  Episode 6: How tech meets travel—redefining va...   \n",
       "2     ['en']  IMMEASURABLY MORE | Ephesians 3:14-21 | Living...   \n",
       "3  ['en-US']  \"The Funeral to End All Funerals\" Season 4 Epi...   \n",
       "4     ['en']                         30 and Pregnant  (Trailer)   \n",
       "\n",
       "                                 episode_description   duration  \\\n",
       "0  In this episode, we discuss the additional cos...  13.906433   \n",
       "1  Travel—the very word is enough to instil energ...  25.466783   \n",
       "2  Paul's prayer for the Ephesians guides us toda...   6.436567   \n",
       "3  Good Janet and Bad Janet unite?! And the Judge...  23.081800   \n",
       "4                                                NaN   0.770133   \n",
       "\n",
       "                 show_id_trans  word_count  \n",
       "0  show_1jUyEaMpjfOYjcwPCdEaec        2507  \n",
       "1  show_0u6NNu3ZyZHyn888FD3WdE        4834  \n",
       "2  show_6KLpvCAxrVzbsnBnRs8O4I         998  \n",
       "3  show_1HvChDzJwUYPX4YU7JJ5Aj        4080  \n",
       "4  show_6rUa8ruUHI2kl7DjyzxBdw         120  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "transcripts_sample = pd.read_csv(\"transcripts_sample.csv.gz\", compression=\"gzip\")\n",
    "print(transcripts_sample.shape)\n",
    "transcripts_sample.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(218118, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>transcript_subset</th>\n",
       "      <th>sentence_enumerated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>35xHkzb4wNPpqipwjAkmDI</td>\n",
       "      <td>Hello and welcome along to the property Academ...</td>\n",
       "      <td>0 - 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>35xHkzb4wNPpqipwjAkmDI</td>\n",
       "      <td>That's absolutely right. So other costs would ...</td>\n",
       "      <td>25 - 50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>35xHkzb4wNPpqipwjAkmDI</td>\n",
       "      <td>And now we insist that they take it it's very ...</td>\n",
       "      <td>50 - 75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>35xHkzb4wNPpqipwjAkmDI</td>\n",
       "      <td>So you could be around sort of four to four an...</td>\n",
       "      <td>75 - 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>35xHkzb4wNPpqipwjAkmDI</td>\n",
       "      <td>That's Urban Butler Dot. Column. So by the tim...</td>\n",
       "      <td>100 - 125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5jYwyaLp8PDnQondFv77kC</td>\n",
       "      <td>Good morning, everyone. This is Trinity here a...</td>\n",
       "      <td>0 - 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5jYwyaLp8PDnQondFv77kC</td>\n",
       "      <td>We were 88 people, right and April we turn hun...</td>\n",
       "      <td>25 - 50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5jYwyaLp8PDnQondFv77kC</td>\n",
       "      <td>So the all the five-year Clauses for Airlines ...</td>\n",
       "      <td>50 - 75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>5jYwyaLp8PDnQondFv77kC</td>\n",
       "      <td>Right for the next five to seven year growth s...</td>\n",
       "      <td>75 - 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>5jYwyaLp8PDnQondFv77kC</td>\n",
       "      <td>How big is the market right travel by default?...</td>\n",
       "      <td>100 - 125</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               episode_id                                  transcript_subset  \\\n",
       "0  35xHkzb4wNPpqipwjAkmDI  Hello and welcome along to the property Academ...   \n",
       "1  35xHkzb4wNPpqipwjAkmDI  That's absolutely right. So other costs would ...   \n",
       "2  35xHkzb4wNPpqipwjAkmDI  And now we insist that they take it it's very ...   \n",
       "3  35xHkzb4wNPpqipwjAkmDI  So you could be around sort of four to four an...   \n",
       "4  35xHkzb4wNPpqipwjAkmDI  That's Urban Butler Dot. Column. So by the tim...   \n",
       "5  5jYwyaLp8PDnQondFv77kC  Good morning, everyone. This is Trinity here a...   \n",
       "6  5jYwyaLp8PDnQondFv77kC  We were 88 people, right and April we turn hun...   \n",
       "7  5jYwyaLp8PDnQondFv77kC  So the all the five-year Clauses for Airlines ...   \n",
       "8  5jYwyaLp8PDnQondFv77kC  Right for the next five to seven year growth s...   \n",
       "9  5jYwyaLp8PDnQondFv77kC  How big is the market right travel by default?...   \n",
       "\n",
       "  sentence_enumerated  \n",
       "0              0 - 25  \n",
       "1             25 - 50  \n",
       "2             50 - 75  \n",
       "3            75 - 100  \n",
       "4           100 - 125  \n",
       "5              0 - 25  \n",
       "6             25 - 50  \n",
       "7             50 - 75  \n",
       "8            75 - 100  \n",
       "9           100 - 125  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract columns for conversion\n",
    "cols_subset = transcripts_sample.loc[: ,[\"episode_id\", \"transcript\"]]\n",
    "\n",
    "\n",
    "# Make dataset with chunked documents on 25 sentences per chunk\n",
    "sentences_25 = sentence_chunk_transcript(cols_subset, chunk_size=25)\n",
    "print(sentences_25.shape)\n",
    "sentences_25.head(10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new dataset\n",
    "sentences_25.to_csv(\"sentences_chunkssize_25.csv.gz\", compression=\"gzip\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
