{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "\n",
    "\n",
    "def tokenize_split(text):\n",
    "    \"\"\"simple tokeniser\"\"\"\n",
    "    return text.split()\n",
    "\n",
    "\n",
    "# Functions for chunking transcripts on either words or sentences\n",
    "\n",
    "def word_chunk_transcript(transcripts, chunk_size=500):\n",
    "    \"\"\" transcripts: must have two columns - episode_id and tokenised_transcript\n",
    "        chunk_size: number of tokens in chunk \n",
    "    \"\"\"\n",
    "    transcripts[\"tokenised_transcript\"] = transcripts.transcript.apply(lambda x: tokenize_split(x))\n",
    "\n",
    "    episode_ls = []\n",
    "    transcript_ls = []\n",
    "    words_enum_ls = [] \n",
    "    for index, row in transcripts.iterrows():\n",
    "        for i in range(0, len(row[\"tokenised_transcript\"]), chunk_size):\n",
    "            episode_ls.append(row[\"episode_id\"])\n",
    "            transcript_ls.append(\" \".join(row[\"tokenised_transcript\"][i:i+chunk_size]))\n",
    "            words_enum_ls.append(f\"{i} - {i+chunk_size}\")\n",
    "    word_chunked_df = pd.DataFrame(data = {'episode_id': episode_ls, 'transcript_subset':transcript_ls, 'words_enumerated':words_enum_ls})\n",
    "    return word_chunked_df\n",
    "\n",
    "\n",
    "def sentence_chunk_transcript(transcripts, chunk_size=20):\n",
    "    \"\"\" transcripts: must have two columns - episode_id and transcript\n",
    "        chunk_size: number of sentences in chunk \n",
    "    \"\"\"\n",
    "    transcripts[\"sentence_token\"] = transcripts.transcript.apply(lambda x: sent_tokenize(x, language='english'))\n",
    "    episode_ls = []\n",
    "    transcript_ls = []\n",
    "    sent_enum_ls = []\n",
    "    for index, row in transcripts.iterrows():\n",
    "        for i in range(0, len(row[\"sentence_token\"]), chunk_size):\n",
    "            episode_ls.append(row[\"episode_id\"])\n",
    "            transcript_ls.append(\" \".join(row[\"sentence_token\"][i:i+chunk_size]))\n",
    "            sent_enum_ls.append(f\"{i} - {i+chunk_size}\")\n",
    "    sentence_chunked_df = pd.DataFrame(data = {'episode_id': episode_ls, 'transcript_subset':transcript_ls, 'sentence_enumerated':sent_enum_ls})\n",
    "    return sentence_chunked_df\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Make chunked dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15000, 16)\n"
     ]
    }
   ],
   "source": [
    "transcripts_sample = pd.read_csv(\"transcripts_sample.csv.gz\", compression=\"gzip\")\n",
    "print(transcripts_sample.shape)\n",
    "# transcripts_sample.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(215064, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>episode_id</th>\n",
       "      <th>transcript_subset</th>\n",
       "      <th>sentence_enumerated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>7tYqM5F5SKtt7lFgcimgAh</td>\n",
       "      <td>I'm Daniel Williams director of active chicks ...</td>\n",
       "      <td>0 - 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>7tYqM5F5SKtt7lFgcimgAh</td>\n",
       "      <td>And when I say, you know, I'm making decisions...</td>\n",
       "      <td>25 - 50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>7tYqM5F5SKtt7lFgcimgAh</td>\n",
       "      <td>What time do you get home who was around you? ...</td>\n",
       "      <td>50 - 75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7tYqM5F5SKtt7lFgcimgAh</td>\n",
       "      <td>And just try to use the process of elimination...</td>\n",
       "      <td>75 - 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>3gaoEuBYb51UoX7zeqv9yr</td>\n",
       "      <td>We recording KP now. We are recording guys pro...</td>\n",
       "      <td>0 - 25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>3gaoEuBYb51UoX7zeqv9yr</td>\n",
       "      <td>So world's greatest agent is all about showcas...</td>\n",
       "      <td>25 - 50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>3gaoEuBYb51UoX7zeqv9yr</td>\n",
       "      <td>Jerk. Well in Los Angeles. Thank you man. Of c...</td>\n",
       "      <td>50 - 75</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>3gaoEuBYb51UoX7zeqv9yr</td>\n",
       "      <td>I even ended up in Special Needs school and I ...</td>\n",
       "      <td>75 - 100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>3gaoEuBYb51UoX7zeqv9yr</td>\n",
       "      <td>We ended up having an hour meeting and we just...</td>\n",
       "      <td>100 - 125</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3gaoEuBYb51UoX7zeqv9yr</td>\n",
       "      <td>What was what do you think he saw in you that ...</td>\n",
       "      <td>125 - 150</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               episode_id                                  transcript_subset  \\\n",
       "0  7tYqM5F5SKtt7lFgcimgAh  I'm Daniel Williams director of active chicks ...   \n",
       "1  7tYqM5F5SKtt7lFgcimgAh  And when I say, you know, I'm making decisions...   \n",
       "2  7tYqM5F5SKtt7lFgcimgAh  What time do you get home who was around you? ...   \n",
       "3  7tYqM5F5SKtt7lFgcimgAh  And just try to use the process of elimination...   \n",
       "4  3gaoEuBYb51UoX7zeqv9yr  We recording KP now. We are recording guys pro...   \n",
       "5  3gaoEuBYb51UoX7zeqv9yr  So world's greatest agent is all about showcas...   \n",
       "6  3gaoEuBYb51UoX7zeqv9yr  Jerk. Well in Los Angeles. Thank you man. Of c...   \n",
       "7  3gaoEuBYb51UoX7zeqv9yr  I even ended up in Special Needs school and I ...   \n",
       "8  3gaoEuBYb51UoX7zeqv9yr  We ended up having an hour meeting and we just...   \n",
       "9  3gaoEuBYb51UoX7zeqv9yr  What was what do you think he saw in you that ...   \n",
       "\n",
       "  sentence_enumerated  \n",
       "0              0 - 25  \n",
       "1             25 - 50  \n",
       "2             50 - 75  \n",
       "3            75 - 100  \n",
       "4              0 - 25  \n",
       "5             25 - 50  \n",
       "6             50 - 75  \n",
       "7            75 - 100  \n",
       "8           100 - 125  \n",
       "9           125 - 150  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Extract columns for conversion\n",
    "cols_subset = transcripts_sample.loc[: ,[\"episode_id\", \"transcript\"]]\n",
    "\n",
    "# Make dataset with chunked documents on 25 sentences per chunk\n",
    "sentences_25 = sentence_chunk_transcript(cols_subset, chunk_size=25)\n",
    "print(sentences_25.shape)\n",
    "sentences_25.head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "400.48517650559836"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check mean word count of chunks\n",
    "sentences_25[\"word_count\"] = sentences_25.transcript_subset.apply(lambda x: len(x.split(\" \")))\n",
    "sentences_25.word_count.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make new dataset\n",
    "sentences_25.to_csv(\"sentences_chunkssize_25.csv.gz\", compression=\"gzip\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(sentences_25[sentences_25.word_count == 1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
