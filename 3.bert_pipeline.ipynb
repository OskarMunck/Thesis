{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First step in pipeline\n",
    "**Load HDBSCAN and t-SNE models and find probabilities for all sentences in a document. The output of this file will be used in the segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openTSNE import TSNE\n",
    "from bertopic import BERTopic\n",
    "from sklearn.decomposition import PCA\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe of one transcript with sentences as instances\n",
    "sentences = pd.read_csv('first_podcast.csv.gz', compression='gzip')\n",
    "documents = sentences.transcript_subset.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dim reduced class object\n",
    "with open('sentence_tsne_data_.pkl', 'rb') as inp:\n",
    "    tsne_data = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cluster model\n",
    "with open('sentence_hdbscan_model.pkl', 'rb') as inp:\n",
    "    hdbscan_model = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedd\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embedded_sentences = sentence_model.encode(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.83% of the variance has been removed by PCA\n",
      "===> Finding 15 nearest neighbors in existing embedding using Annoy approximate search...\n",
      "   --> Time elapsed: 11.78 seconds\n",
      "===> Calculating affinity matrix...\n",
      "   --> Time elapsed: 0.03 seconds\n",
      "===> Running optimization with exaggeration=4.00, lr=0.10 for 0 iterations...\n",
      "   --> Time elapsed: 0.00 seconds\n",
      "===> Running optimization with exaggeration=1.50, lr=0.10 for 250 iterations...\n",
      "Iteration   50, KL divergence 9527.4070, 50 iterations in 70.7268 sec\n",
      "Iteration  100, KL divergence 9287.1780, 50 iterations in 92.7301 sec\n",
      "Iteration  150, KL divergence 9191.3463, 50 iterations in 104.2941 sec\n",
      "Iteration  200, KL divergence 9151.7852, 50 iterations in 87.6444 sec\n",
      "Iteration  250, KL divergence 9132.1676, 50 iterations in 86.1700 sec\n",
      "   --> Time elapsed: 441.57 seconds\n"
     ]
    }
   ],
   "source": [
    "# reduce dimensions with PCA and t-SNE\n",
    "PCA_model = PCA(n_components = 50)\n",
    "PCA_data = PCA_model.fit_transform(embedded_sentences)\n",
    "print(f\"{1-sum(PCA_model.explained_variance_ratio_):.2%} of the variance has been removed by PCA\")\n",
    "\n",
    "tsne_test = tsne_data.transform(PCA_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oskarmunckafrosenschold/Documents/Thesis/thesisenv/lib/python3.10/site-packages/hdbscan/prediction.py:581: RuntimeWarning: All-NaN slice encountered\n",
      "  outlier_vec = outlier_membership_vector(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrows: 599\n",
      "ncols: 156\n"
     ]
    }
   ],
   "source": [
    "# get probability vectors for each cluster via soft clustering\n",
    "probabilities = hdbscan.membership_vector(hdbscan_model, tsne_test)\n",
    "print(f'nrows: {len(probabilities)}')\n",
    "print(f'ncols: {len(probabilities[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nan rows: 2\n"
     ]
    }
   ],
   "source": [
    "# temp remedy for HDBSCAN problem\n",
    "inds = np.where(np.isnan(probabilities))\n",
    "probabilities[inds] = 0\n",
    "print(f\"Number of nan rows: {len(set(list(inds[:][0])))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save probabilities to use in segmentation\n",
    "np.save('probabilities.npy', probabilities)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deprecated code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "topic_model = BERTopic.load('pipeline_BERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get probabilities for each sentence of each topic\n",
    "_, probabilities = topic_model.transform(documents, tsne_data)\n",
    "print(f'nrows: {len(probabilities)}')\n",
    "print(f'ncols: {len(probabilities[0])}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
