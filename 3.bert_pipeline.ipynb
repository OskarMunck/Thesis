{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First step in pipeline\n",
    "**Load HDBSCAN and t-SNE models and find probabilities for all sentences in a document. The output of this file will be used in the segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openTSNE import TSNE\n",
    "from bertopic import BERTopic\n",
    "from sklearn.decomposition import PCA\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe of one transcript with sentences as instances\n",
    "sentences = pd.read_csv('prediction_data.csv')\n",
    "documents = sentences.transcript_subset.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dim reduced class object\n",
    "with open('sentence_tsne_data_.pkl', 'rb') as inp:\n",
    "    tsne_data = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cluster model\n",
    "with open('sentence_hdbscan_model.pkl', 'rb') as inp:\n",
    "    hdbscan_model = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedd\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embedded_sentences = sentence_model.encode(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35.88% of the variance has been removed by PCA\n",
      "===> Finding 15 nearest neighbors in existing embedding using Annoy approximate search...\n",
      "   --> Time elapsed: 4.66 seconds\n",
      "===> Calculating affinity matrix...\n",
      "   --> Time elapsed: 0.02 seconds\n",
      "===> Running optimization with exaggeration=4.00, lr=0.10 for 0 iterations...\n",
      "   --> Time elapsed: 0.00 seconds\n",
      "===> Running optimization with exaggeration=1.50, lr=0.10 for 250 iterations...\n",
      "Iteration   50, KL divergence 7340.2406, 50 iterations in 31.4019 sec\n",
      "Iteration  100, KL divergence 7178.4666, 50 iterations in 32.1260 sec\n",
      "Iteration  150, KL divergence 7119.3947, 50 iterations in 31.5695 sec\n",
      "Iteration  200, KL divergence 7078.9133, 50 iterations in 29.5806 sec\n",
      "Iteration  250, KL divergence 7058.9904, 50 iterations in 30.2641 sec\n",
      "   --> Time elapsed: 154.94 seconds\n"
     ]
    }
   ],
   "source": [
    "# reduce dimensions with PCA and t-SNE\n",
    "PCA_model = PCA(n_components = 50)\n",
    "PCA_data = PCA_model.fit_transform(embedded_sentences)\n",
    "print(f\"{1-sum(PCA_model.explained_variance_ratio_):.2%} of the variance has been removed by PCA\")\n",
    "\n",
    "tsne_test = tsne_data.transform(PCA_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oskarmunckafrosenschold/Documents/Thesis/thesisenv/lib/python3.10/site-packages/hdbscan/prediction.py:581: RuntimeWarning: All-NaN slice encountered\n",
      "  outlier_vec = outlier_membership_vector(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrows: 468\n",
      "ncols: 156\n"
     ]
    }
   ],
   "source": [
    "# get probability vectors for each cluster via soft clustering\n",
    "probabilities = hdbscan.membership_vector(hdbscan_model, tsne_test)\n",
    "print(f'nrows: {len(probabilities)}')\n",
    "print(f'ncols: {len(probabilities[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of nan rows: 1\n"
     ]
    }
   ],
   "source": [
    "# temp remedy for HDBSCAN problem\n",
    "inds = np.where(np.isnan(probabilities))\n",
    "probabilities[inds] = 0\n",
    "print(f\"Number of nan rows: {len(set(list(inds[:][0])))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save probabilities to use in segmentation\n",
    "np.save('probabilities.npy', probabilities)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deprecated code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "topic_model = BERTopic.load('pipeline_BERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get probabilities for each sentence of each topic\n",
    "_, probabilities = topic_model.transform(documents, tsne_data)\n",
    "print(f'nrows: {len(probabilities)}')\n",
    "print(f'ncols: {len(probabilities[0])}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
