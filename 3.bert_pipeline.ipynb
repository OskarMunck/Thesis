{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First step in pipeline\n",
    "**Load HDBSCAN and t-SNE models and find probabilities for all sentences in a document. The output of this file will be used in the segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openTSNE import TSNE\n",
    "from bertopic import BERTopic\n",
    "from sklearn.decomposition import PCA\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe of one transcript with sentences as instances\n",
    "sentences = pd.read_csv('prediction_data.csv')\n",
    "documents = sentences.transcript_subset.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dim reduced class object\n",
    "with open('tsne_data.pkl', 'rb') as inp:\n",
    "    tsne_data = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cluster model\n",
    "with open('hdbscan_model.pkl', 'rb') as inp:\n",
    "    hdbscan_model = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedd\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embedded_sentences = sentence_model.encode(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Finding 15 nearest neighbors in existing embedding using Annoy approximate search...\n",
      "   --> Time elapsed: 0.14 seconds\n",
      "===> Calculating affinity matrix...\n",
      "   --> Time elapsed: 0.01 seconds\n",
      "===> Running optimization with exaggeration=4.00, lr=0.10 for 0 iterations...\n",
      "   --> Time elapsed: 0.00 seconds\n",
      "===> Running optimization with exaggeration=1.50, lr=0.10 for 250 iterations...\n",
      "Iteration   50, KL divergence 7073.1264, 50 iterations in 19.4517 sec\n",
      "Iteration  100, KL divergence 6892.8614, 50 iterations in 16.9902 sec\n",
      "Iteration  150, KL divergence 6838.9759, 50 iterations in 16.8900 sec\n",
      "Iteration  200, KL divergence 6805.9688, 50 iterations in 16.5368 sec\n",
      "Iteration  250, KL divergence 6789.3724, 50 iterations in 16.3548 sec\n",
      "   --> Time elapsed: 86.22 seconds\n"
     ]
    }
   ],
   "source": [
    "# reduce dimensions with PCA and t-SNE\n",
    "PCA_data = PCA(n_components = 50).fit_transform(embedded_sentences)\n",
    "\n",
    "tsne_test = tsne_data.transform(PCA_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrows: 468\n",
      "ncols: 256\n"
     ]
    }
   ],
   "source": [
    "# get probability vectors for each cluster via soft clustering\n",
    "probabilities = hdbscan.membership_vector(hdbscan_model, tsne_test)\n",
    "print(f'nrows: {len(probabilities)}')\n",
    "print(f'ncols: {len(probabilities[0])}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save probabilities to use in segmentation\n",
    "np.save('probabilities.npy', probabilities)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deprecated code "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model\n",
    "topic_model = BERTopic.load('pipeline_BERT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get probabilities for each sentence of each topic\n",
    "_, probabilities = topic_model.transform(documents, tsne_data)\n",
    "print(f'nrows: {len(probabilities)}')\n",
    "print(f'ncols: {len(probabilities[0])}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
