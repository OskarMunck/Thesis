{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First step in pipeline\n",
    "**Load HDBSCAN and t-SNE models and find probabilities for all sentences in a document. The output of this file will be used in the segmentation**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "import hdbscan\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from openTSNE import TSNE\n",
    "from bertopic import BERTopic\n",
    "from sklearn.decomposition import PCA\n",
    "from sentence_transformers import SentenceTransformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load PCA class object\n",
    "with open('pca_model_100.pkl', 'rb') as inp:\n",
    "    PCA_model = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load t-SNE class object\n",
    "with open('sentence100_tsne_data_.pkl', 'rb') as inp:\n",
    "    tsne_data = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load cluster model\n",
    "with open('sentence100_hdbscan_model.pkl', 'rb') as inp:\n",
    "    hdbscan_model = pickle.load(inp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through all of the annotated transcripts and save their topic probability distribution\n",
    "directory = '../Thesis/annotated_transcripts_input'\n",
    "\n",
    "for dirpath, _, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        episode_id = file.split('_')[0]\n",
    "        path = os.path.join(dirpath, file)\n",
    "        with open(path, errors='replace') as f: \n",
    "            prediction_documents = pd.read_csv(f)\n",
    "            docs = prediction_documents.transcript_subset\n",
    "\n",
    "            # embedd\n",
    "            sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "            embedded_sentences = sentence_model.encode(docs)\n",
    "\n",
    "            # reduce dimensions with PCA and t-SNE\n",
    "            PCA_data = PCA_model.transform(embedded_sentences)\n",
    "            print(f\"{1-sum(PCA_model.explained_variance_ratio_):.2%} of the variance has been removed by PCA for transcript {episode_id}\")\n",
    "            tsne_prediction = tsne_data.transform(PCA_data)\n",
    "\n",
    "            # get probability vectors for each cluster via soft clustering\n",
    "            probabilities = hdbscan.membership_vector(hdbscan_model, tsne_prediction)\n",
    "            print(f'nrows: {len(probabilities)}\\nncols: {len(probabilities[0])}')\n",
    "            # remedy HDBSCAN problem\n",
    "            inds = np.where(np.isnan(probabilities))\n",
    "            probabilities[inds] = 0\n",
    "            print(f\"Number of nan rows: {len(set(list(inds[:][0])))} for {episode_id}\\n\\n\")\n",
    "\n",
    "            # save probabilities to use in downstream segmentation\n",
    "            name_of_file = f'../Thesis/annotated_probabilities/topic_probability_density_vector_{episode_id}.npy'\n",
    "            np.save(name_of_file, probabilities)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Code for individual transcript predicitons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load dataframe of one transcript with sentences as instances\n",
    "sentences = pd.read_csv('first_podcast.csv.gz', compression='gzip')\n",
    "documents = sentences.transcript_subset.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedd\n",
    "sentence_model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "embedded_sentences = sentence_model.encode(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "38.83% of the variance has been removed by PCA\n",
      "===> Finding 15 nearest neighbors in existing embedding using Annoy approximate search...\n",
      "   --> Time elapsed: 11.78 seconds\n",
      "===> Calculating affinity matrix...\n",
      "   --> Time elapsed: 0.03 seconds\n",
      "===> Running optimization with exaggeration=4.00, lr=0.10 for 0 iterations...\n",
      "   --> Time elapsed: 0.00 seconds\n",
      "===> Running optimization with exaggeration=1.50, lr=0.10 for 250 iterations...\n",
      "Iteration   50, KL divergence 9527.4070, 50 iterations in 70.7268 sec\n",
      "Iteration  100, KL divergence 9287.1780, 50 iterations in 92.7301 sec\n",
      "Iteration  150, KL divergence 9191.3463, 50 iterations in 104.2941 sec\n",
      "Iteration  200, KL divergence 9151.7852, 50 iterations in 87.6444 sec\n",
      "Iteration  250, KL divergence 9132.1676, 50 iterations in 86.1700 sec\n",
      "   --> Time elapsed: 441.57 seconds\n"
     ]
    }
   ],
   "source": [
    "# reduce dimensions with PCA and t-SNE\n",
    "PCA_model = PCA(n_components = 50)\n",
    "PCA_data = PCA_model.fit_transform(embedded_sentences)\n",
    "print(f\"{1-sum(PCA_model.explained_variance_ratio_):.2%} of the variance has been removed by PCA\")\n",
    "\n",
    "tsne_test = tsne_data.transform(PCA_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oskarmunckafrosenschold/Documents/Thesis/thesisenv/lib/python3.10/site-packages/hdbscan/prediction.py:581: RuntimeWarning: All-NaN slice encountered\n",
      "  outlier_vec = outlier_membership_vector(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nrows: 599\n",
      "ncols: 156\n"
     ]
    }
   ],
   "source": [
    "# get probability vectors for each cluster via soft clustering\n",
    "probabilities = hdbscan.membership_vector(hdbscan_model, tsne_test)\n",
    "print(f'nrows: {len(probabilities)}')\n",
    "print(f'ncols: {len(probabilities[0])}')\n",
    "# remedy for HDBSCAN problem\n",
    "inds = np.where(np.isnan(probabilities))\n",
    "probabilities[inds] = 0\n",
    "print(f\"Number of nan rows: {len(set(list(inds[:][0])))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save probabilities to use in segmentation\n",
    "np.save('probabilities.npy', probabilities)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
